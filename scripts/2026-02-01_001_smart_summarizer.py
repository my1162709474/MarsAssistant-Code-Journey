#!/usr/bin/env python3
"""
Day 22: æ™ºèƒ½æ–‡æœ¬æ‘˜è¦å™¨ (Smart Text Summarizer)

è¿™æ˜¯ä¸€ä¸ªåŸºäºæŠ½å–å¼æ–¹æ³•çš„ç®€å•æ–‡æœ¬æ‘˜è¦å·¥å…·ï¼Œ
å±•ç¤ºäº†è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºæœ¬æ¦‚å¿µã€‚
"""

import re
from collections import Counter
from typing import List, Tuple


class SmartSummarizer:
    """æ™ºèƒ½æ–‡æœ¬æ‘˜è¦å™¨ç±»"""
    
    def __init__(self):
        self.stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',
            'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰',
            'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'that', 'the', 'is', 'a', 'an', 'and', 'or',
            'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        }
    
    def preprocess(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
        words = re.findall(r'\b\w+\b', text.lower())
        # ç§»é™¤åœç”¨è¯
        words = [w for w in words if w not in self.stop_words and len(w) > 1]
        return words
    
    def calculate_sentence_scores(self, text: str) -> List[Tuple[int, float]]:
        """è®¡ç®—æ¯ä¸ªå¥å­çš„é‡è¦æ€§å¾—åˆ†"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        sentence_scores = []
        for idx, sentence in enumerate(sentences):
            words = self.tokenize(sentence)
            
            if not words:
                continue
            
            # åŸºäºè¯é¢‘è®¡ç®—å¾—åˆ†
            word_freq = Counter(words)
            max_freq = max(word_freq.values()) if word_freq else 1
            
            # å½’ä¸€åŒ–å¾—åˆ†
            score = sum(word_freq[word] / max_freq for word in words)
            
            # å¥å­é•¿åº¦æƒ©ç½šï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿çš„å¥å­å¾—åˆ†é™ä½ï¼‰
            length_penalty = 1.0
            if len(words) < 5:
                length_penalty = 0.5
            elif len(words) > 50:
                length_penalty = 0.8
            
            score *= length_penalty
            sentence_scores.append((idx, score))
        
        return sentence_scores
    
    def summarize(self, text: str, ratio: float = 0.3) -> str:
        """
        ç”Ÿæˆæ‘˜è¦
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            ratio: æ‘˜è¦é•¿åº¦æ¯”ä¾‹ (é»˜è®¤30%)
        
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        text = self.preprocess(text)
        sentence_scores = self.calculate_sentence_scores(text)
        
        if not sentence_scores:
            return ""
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œé€‰æ‹©å‰Nä¸ªå¥å­
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        num_sentences = max(1, int(len(sentence_scores) * ratio))
        top_sentences = sentence_scores[:num_sentences]
        
        # æŒ‰åŸæ–‡é¡ºåºé‡æ–°æ’åˆ—
        top_sentences.sort(key=lambda x: x[0])
        
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        summary = ''.join(sentences[i[0]] for i in top_sentences)
        return summary
    
    def extract_keywords(self, text: str, top_n: int = 5) -> List[str]:
        """æå–å…³é”®è¯"""
        words = self.tokenize(text)
        word_freq = Counter(words)
        return [word for word, _ in word_freq.most_common(top_n)]


def demo():
    """æ¼”ç¤ºæ‘˜è¦åŠŸèƒ½"""
    
    sample_text = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
    å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
    ç ”ç©¶èŒƒå›´åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªé¢†åŸŸã€‚
    è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•æ¨åŠ¨äº†AIçš„å¿«é€Ÿè¿›æ­¥ï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€
    è‡ªç„¶è¯­è¨€ç†è§£ç­‰æ–¹é¢å–å¾—äº†çªç ´æ€§æˆæœã€‚AIæŠ€æœ¯æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œ
    ä»æ™ºèƒ½åŠ©æ‰‹åˆ°è‡ªåŠ¨é©¾é©¶ï¼Œä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èåˆ†æï¼ŒAIçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚
    ç„¶è€Œï¼ŒAIçš„å‘å±•ä¹Ÿå¸¦æ¥äº†ä¼¦ç†å’Œå®‰å…¨æ–¹é¢çš„æŒ‘æˆ˜ï¼Œéœ€è¦ç¤¾ä¼šå„ç•Œå…±åŒåŠªåŠ›ã€‚
    """
    
    print("=" * 60)
    print("Day 22: æ™ºèƒ½æ–‡æœ¬æ‘˜è¦å™¨æ¼”ç¤º")
    print("=" * 60)
    
    summarizer = SmartSummarizer()
    
    # ç”Ÿæˆæ‘˜è¦
    summary = summarizer.summarize(sample_text, ratio=0.4)
    print("\nğŸ“ åŸæ–‡æ‘˜è¦ï¼š")
    print(summary)
    
    # æå–å…³é”®è¯
    keywords = summarizer.extract_keywords(sample_text)
    print(f"\nğŸ”‘ å…³é”®è¯ï¼š{', '.join(keywords)}")
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    demo()

#!/usr/bin/env python3
"""
æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å™¨ - Day 8: Text Similarity Calculator

æ”¯æŒå¤šç§ç›¸ä¼¼åº¦ç®—æ³•ï¼š
1. ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)
2. Jaccardç›¸ä¼¼åº¦
3. Levenshteinç¼–è¾‘è·ç¦»
4. SimHash (ç”¨äºå¤§è§„æ¨¡æ–‡æœ¬å»é‡)

Author: AI Assistant
Date: 2026-02-01
"""

import math
from collections import Counter
import hashlib

def tokenize(text):
    """ç®€å•åˆ†è¯"""
    return text.lower().split()

def cosine_similarity(text1, text2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    words1 = set(tokenize(text1))
    words2 = set(tokenize(text2))
    
    all_words = words1 | words2
    vec1 = [1 if w in words1 else 0 for w in all_words]
    vec2 = [1 if w in words2 else 0 for w in all_words]
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(a * a for a in vec2))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def jaccard_similarity(text1, text2):
    """è®¡ç®—Jaccardç›¸ä¼¼åº¦"""
    words1 = set(tokenize(text1))
    words2 = set(tokenize(text2))
    
    intersection = words1 & words2
    union = words1 | words2
    
    if len(union) == 0:
        return 1.0
    return len(intersection) / len(union)

def levenshtein_distance(s1, s2):
    """è®¡ç®—Levenshteinç¼–è¾‘è·ç¦»"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def normalized_levenshtein(s1, s2):
    """å½’ä¸€åŒ–Levenshteinç›¸ä¼¼åº¦"""
    distance = levenshtein_distance(s1, s2)
    max_len = max(len(s1), len(s2))
    if max_len == 0:
        return 1.0
    return 1 - (distance / max_len)

def simhash(text, fingerprint_size=64):
    """SimHashç®—æ³• - ç”¨äºå¤§è§„æ¨¡æ–‡æœ¬å»é‡"""
    words = text.lower().split()
    shingles = []
    
    for i in range(len(words) - 1):
        shingle = ' '.join(words[i:i+2])
        shingles.append(shingle)
    
    if not shingles:
        shingles = words
    
    hash_vectors = []
    for shingle in shingles:
        h = int(hashlib.md5(shingle.encode()).hexdigest(), 16)
        hash_vectors.append([1 if (h >> i) & 1 else -1 for i in range(fingerprint_size)])
    
    fingerprint = [sum(v[i] for v in hash_vectors) for i in range(fingerprint_size)]
    result = sum(1 << i if fingerprint[i] > 0 else 0 for i in range(fingerprint_size))
    
    return result

def hamming_distance(hash1, hash2):
    """è®¡ç®—SimHashçš„æµ·æ˜è·ç¦»"""
    return bin(hash1 ^ hash2).count('1')

class TextSimilarity:
    """æ–‡æœ¬ç›¸ä¼¼åº¦ç»¼åˆå·¥å…·ç±»"""
    
    def __init__(self):
        self.results = {}
    
    def analyze(self, text1, text2):
        """å…¨é¢åˆ†æä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        self.results = {
            'cosine': cosine_similarity(text1, text2),
            'jaccard': jaccard_similarity(text1, text2),
            'levenshtein_norm': normalized_levenshtein(text1, text2),
        }
        
        hash1 = simhash(text1)
        hash2 = simhash(text2)
        self.results['simhash_hamming'] = hamming_distance(hash1, hash2)
        
        return self.results
    
    def get_similarity_report(self, text1, text2):
        """ç”Ÿæˆç›¸ä¼¼åº¦æŠ¥å‘Š"""
        results = self.analyze(text1, text2)
        
        report = f"""
=== æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š ===

ğŸ“Š ç›¸ä¼¼åº¦å¾—åˆ†:
  â€¢ ä½™å¼¦ç›¸ä¼¼åº¦: {results['cosine']:.4f}
  â€¢ Jaccardç›¸ä¼¼åº¦: {results['jaccard']:.4f}
  â€¢ å½’ä¸€åŒ–ç¼–è¾‘è·ç¦»: {results['levenshtein_norm']:.4f}
  â€¢ SimHashæµ·æ˜è·ç¦»: {results['simhash_hamming']}

ğŸ’¡ è§£è¯»:
  â€¢ ä½™å¼¦ç›¸ä¼¼åº¦: è¶Šé«˜è¶Šç›¸ä¼¼ (èŒƒå›´: 0-1)
  â€¢ Jaccardç›¸ä¼¼åº¦: è¶Šé«˜è¶Šç›¸ä¼¼ (èŒƒå›´: 0-1)
  â€¢ ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦: è¶Šé«˜è¶Šç›¸ä¼¼ (èŒƒå›´: 0-1)
  â€¢ SimHashè·ç¦»: è¶Šä½è¶Šç›¸ä¼¼ (èŒƒå›´: 0-{64})
"""
        return report

def demo():
    """æ¼”ç¤º"""
    text1 = "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥æœ€æœ‰å‰é€”çš„é¢†åŸŸä¹‹ä¸€"
    text2 = "AIæ˜¯æœªæ¥æœ€æœ‰å‘å±•å‰æ™¯çš„æŠ€æœ¯æ–¹å‘"
    text3 = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œé€‚åˆå»å…¬å›­æ•£æ­¥"
    
    print("=" * 50)
    print("ğŸ¯ æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å™¨æ¼”ç¤º")
    print("=" * 50)
    
    analyzer = TextSimilarity()
    
    print("\nğŸ“ å¯¹æ¯”1 - ç›¸å…³ä¸»é¢˜:")
    print(f"æ–‡æœ¬1: {text1}")
    print(f"æ–‡æœ¬2: {text2}")
    print(analyzer.get_similarity_report(text1, text2))
    
    print("\nğŸ“ å¯¹æ¯”2 - ä¸ç›¸å…³ä¸»é¢˜:")
    print(f"æ–‡æœ¬1: {text1}")
    print(f"æ–‡æœ¬3: {text3}")
    print(analyzer.get_similarity_report(text1, text3))

if __name__ == "__main__":
    demo()

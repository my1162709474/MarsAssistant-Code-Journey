"""
AIå¯¹è¯åˆ†æå™¨ - Chat Analyzer
============================
åŠŸèƒ½ï¼š
1. è¯é¢‘ç»Ÿè®¡
2. å¯¹è¯æƒ…æ„Ÿå€¾å‘åˆ†æ
3. è¯é¢˜æå–
4. å¯¹è¯æ‘˜è¦ç”Ÿæˆ
5. æ´»è·ƒåº¦ç»Ÿè®¡

ä½œè€…: MarsAssistant
æ—¥æœŸ: 2026-02-01
"""

import re
import json
from collections import Counter
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import hashlib


class ChatAnalyzer:
    """AIå¯¹è¯åˆ†æå™¨ç±»"""
    
    def __init__(self, chat_history: List[Dict]):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            chat_history: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« role, content, timestamp
        """
        self.chat_history = chat_history
        self.messages = [msg.get('content', '') for msg in chat_history if msg.get('content')]
        self.roles = [msg.get('role', 'unknown') for msg in chat_history]
        
        # å¸¸ç”¨åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä¹ˆ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™æ ·', 'é‚£æ ·', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥',
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',
            'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
            'and', 'or', 'but', 'if', 'then', 'that', 'this', 'these', 'those', 'it', 'its'
        }
    
    def word_frequency(self, top_n: int = 20) -> List[Tuple[str, int]]:
        """ç»Ÿè®¡è¯é¢‘"""
        # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯
        text = ' '.join(self.messages)
        
        # æå–ä¸­æ–‡å’Œè‹±æ–‡å•è¯
        chinese = re.findall(r'[\u4e00-\u9fa5]+', text)
        english = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯
        chinese_words = [w for w in chinese if len(w) > 1 and w not in self.stop_words]
        english_words = [w for w in english if len(w) > 2 and w not in self.stop_words]
        
        # åˆå¹¶è®¡æ•°
        all_words = chinese_words + english_words
        counter = Counter(all_words)
        
        return counter.most_common(top_n)
    
    def role_distribution(self) -> Dict[str, int]:
        """è§’è‰²åˆ†å¸ƒç»Ÿè®¡"""
        return Counter(self.roles)
    
    def message_length_stats(self) -> Dict[str, float]:
        """æ¶ˆæ¯é•¿åº¦ç»Ÿè®¡"""
        if not self.messages:
            return {'avg': 0, 'max': 0, 'min': 0, 'total': 0}
        
        lengths = [len(msg) for msg in self.messages]
        return {
            'avg': sum(lengths) / len(lengths),
            'max': max(lengths),
            'min': min(lengths),
            'total': sum(lengths)
        }
    
    def extract_topics(self, num_topics: int = 5) -> List[str]:
        """æå–è¯é¢˜å…³é”®è¯"""
        word_freq = self.word_frequency(50)
        
        # è¿‡æ»¤å•å­—è¯
        meaningful_words = [(w, c) for w, c in word_freq if len(w) >= 2]
        
        return [word for word, count in meaningful_words[:num_topics]]
    
    def calculate_engagement_score(self) -> float:
        """è®¡ç®—äº’åŠ¨æ´»è·ƒåº¦åˆ†æ•°"""
        if not self.chat_history:
            return 0.0
        
        # åŸºç¡€åˆ†ï¼šå¯¹è¯è½®æ•°
        base_score = min(len(self.chat_history) * 2, 50)
        
        # æ¶ˆæ¯é•¿åº¦å¾—åˆ†
        length_stats = self.message_length_stats()
        avg_length = length_stats['avg']
        length_score = min(avg_length / 10, 20)  # æœ€å¤š20åˆ†
        
        # è¯æ±‡ä¸°å¯Œåº¦å¾—åˆ†
        word_count = len(set(' '.join(self.messages).split()))
        vocabulary_score = min(word_count / 5, 15)  # æœ€å¤š15åˆ†
        
        # è§’è‰²å¤šæ ·æ€§å¾—åˆ†
        role_count = len(set(self.roles))
        role_score = min(role_count * 5, 15)  # æœ€å¤š15åˆ†
        
        total_score = base_score + length_score + vocabulary_score + role_score
        return round(total_score, 2)
    
    def generate_summary(self) -> Dict:
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        return {
            'total_messages': len(self.chat_history),
            'date_range': self._get_date_range(),
            'top_words': dict(self.word_frequency(10)),
            'role_distribution': dict(self.role_distribution()),
            'engagement_score': self.calculate_engagement_score(),
            'topics': self.extract_topics(),
            'avg_message_length': round(self.message_length_stats()['avg'], 2)
        }
    
    def _get_date_range(self) -> Optional[Tuple[str, str]]:
        """è·å–å¯¹è¯æ—¥æœŸèŒƒå›´"""
        timestamps = []
        for msg in self.chat_history:
            if 'timestamp' in msg:
                try:
                    ts = datetime.fromisoformat(msg['timestamp'].replace('Z', '+00:00'))
                    timestamps.append(ts)
                except:
                    pass
        
        if not timestamps:
            return None
        
        timestamps.sort()
        return (timestamps[0].strftime('%Y-%m-%d %H:%M'), 
                timestamps[-1].strftime('%Y-%m-%d %H:%M'))
    
    def export_analysis(self, output_path: str = 'analysis_report.json'):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'summary': self.generate_summary(),
            'detailed_stats': {
                'word_frequency': self.word_frequency(30),
                'message_lengths': self.message_length_stats(),
                'role_stats': self.role_distribution()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"åˆ†ææŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")
        return report


# ç¤ºä¾‹ä½¿ç”¨
def demo():
    """æ¼”ç¤ºç”¨æ³•"""
    # æ¨¡æ‹Ÿå¯¹è¯å†å²
    sample_history = [
        {'role': 'user', 'content': 'ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘æƒ³å»å…¬å›­æ•£æ­¥', 'timestamp': '2026-02-01T10:00:00Z'},
        {'role': 'assistant', 'content': 'å¬èµ·æ¥æ˜¯ä¸ªä¸é”™çš„ä¸»æ„ï¼å»ºè®®ä½ å»é™„è¿‘çš„å°å…¬å›­èµ°èµ°ï¼Œå‘¼å¸æ–°é²œç©ºæ°”å¯¹èº«å¿ƒéƒ½æœ‰ç›Šå¤„ã€‚', 'timestamp': '2026-02-01T10:01:00Z'},
        {'role': 'user', 'content': 'ä½ èƒ½å¸®æˆ‘å†™ä¸€ä¸ªPythonè„šæœ¬æ¥åˆ†æè¿™äº›å¯¹è¯è®°å½•å—ï¼Ÿ', 'timestamp': '2026-02-01T10:05:00Z'},
        {'role': 'assistant', 'content': 'å½“ç„¶å¯ä»¥ï¼æˆ‘åˆšåˆšä¸ºä½ åˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ChatAnalyzerç±»ï¼Œå®ƒåŒ…å«è¯é¢‘ç»Ÿè®¡ã€è¯é¢˜æå–ã€æ´»è·ƒåº¦åˆ†æç­‰åŠŸèƒ½ã€‚', 'timestamp': '2026-02-01T10:06:00Z'},
        {'role': 'user', 'content': 'å¤ªæ£’äº†ï¼è¿™ä¸ªè„šæœ¬åŠŸèƒ½å¾ˆä¸°å¯Œï¼Œæˆ‘è¿˜æƒ³æ·»åŠ ä¸€ä¸ªæƒ…æ„Ÿåˆ†æåŠŸèƒ½', 'timestamp': '2026-02-01T10:08:00Z'},
        {'role': 'assistant', 'content': 'å¥½å»ºè®®ï¼æƒ…æ„Ÿåˆ†æå¯ä»¥é€šè¿‡å…³é”®è¯åŒ¹é…æˆ–è€…è°ƒç”¨ä¸“é—¨çš„NLP APIæ¥å®ç°ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­åŠ å…¥è¿™ä¸ªåŠŸèƒ½ã€‚', 'timestamp': '2026-02-01T10:09:00Z'},
    ]
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ChatAnalyzer(sample_history)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print("=" * 50)
    print("AIå¯¹è¯åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  - æ€»æ¶ˆæ¯æ•°: {len(analyzer.chat_history)}")
    
    print(f"\nğŸ“ è¯é¢‘ç»Ÿè®¡ (Top 10):")
    for word, count in analyzer.word_frequency(10):
        print(f"  {word}: {count}")
    
    print(f"\nğŸ¯ è¯é¢˜æå–:")
    print(f"  {analyzer.extract_topics()}")
    
    print(f"\nğŸ’¬ è§’è‰²åˆ†å¸ƒ:")
    for role, count in analyzer.role_distribution().items():
        print(f"  {role}: {count}")
    
    print(f"\nâ­ äº’åŠ¨æ´»è·ƒåº¦: {analyzer.calculate_engagement_score()}")
    
    print(f"\nğŸ“‹ å®Œæ•´æ‘˜è¦:")
    print(json.dumps(analyzer.generate_summary(), ensure_ascii=False, indent=2))
    
    # å¯¼å‡ºæŠ¥å‘Š
    analyzer.export_analysis()
    
    return analyzer


if __name__ == '__main__':
    demo()

#!/usr/bin/env python3
"""
æ–‡ä»¶å“ˆå¸ŒéªŒè¯å·¥å…· (Day 38)
æ”¯æŒå¤šç§å“ˆå¸Œç®—æ³•ï¼Œæ‰¹é‡éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼Œç”Ÿæˆæ ¡éªŒå’Œæ–‡ä»¶

åŠŸèƒ½:
- MD5, SHA-1, SHA-256, SHA-512 æ”¯æŒ
- æ‰¹é‡æ–‡ä»¶å¤„ç†å’Œç›®å½•é€’å½’
- æ ¡éªŒå’Œæ–‡ä»¶ç”Ÿæˆä¸éªŒè¯
- å¢é‡éªŒè¯æ¨¡å¼
"""

import hashlib
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import sys


class FileHasher:
    """æ–‡ä»¶å“ˆå¸Œå¤„ç†å™¨"""
    
    ALGORITHMS = {
        'md5': hashlib.md5,
        'sha1': hashlib.sha1,
        'sha256': hashlib.sha256,
        'sha512': hashlib.sha512
    }
    
    def __init__(self, algorithm: str = 'sha256'):
        if algorithm not in self.ALGORITHMS:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}ï¼Œå¯é€‰: {list(self.ALGORITHMS.keys())}")
        self.algorithm = algorithm
    
    def hash_file(self, filepath: str, chunk_size: int = 65536) -> str:
        """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
        hasher = self.ALGORITHMS[self.algorithm]()
        file_size = os.path.getsize(filepath)
        
        with open(filepath, 'rb') as f:
            processed = 0
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
                processed += len(chunk)
                # æ˜¾ç¤ºè¿›åº¦
                progress = (processed / file_size) * 100
                sys.stdout.write(f"\r  è¿›åº¦: {progress:.1f}%")
                sys.stdout.flush()
        
        print()  # æ¢è¡Œ
        return hasher.hexdigest()
    
    def hash_directory(self, directory: str, recursive: bool = True,
                       patterns: Optional[List[str]] = None) -> Dict[str, str]:
        """è®¡ç®—ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
        results = {}
        directory = Path(directory)
        
        if patterns is None:
            patterns = ['*']
        
        def should_include(path: Path) -> bool:
            if path.is_file():
                for pattern in patterns:
                    if path.match(pattern):
                        return True
            return False
        
        iterator = directory.rglob('*') if recursive else directory.glob('*')
        
        for item in iterator:
            if item.is_file() and should_include(item):
                rel_path = str(item.relative_to(directory))
                results[rel_path] = self.hash_file(str(item))
        
        return results


class ChecksumManager:
    """æ ¡éªŒå’Œæ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, hasher: FileHasher):
        self.hasher = hasher
    
    def generate_checksum_file(self, files: Dict[str, str], output_path: str,
                                relative_base: Optional[str] = None) -> str:
        """ç”Ÿæˆæ ¡éªŒå’Œæ–‡ä»¶"""
        checksums = {
            'algorithm': self.hasher.algorithm,
            'generated_at': datetime.now().isoformat(),
            'files': {}
        }
        
        for filepath, hash_value in files.items():
            if relative_base:
                rel_path = os.path.relpath(filepath, relative_base)
            else:
                rel_path = filepath
            checksums['files'][rel_path] = hash_value
        
        # å†™å…¥JSONæ ¼å¼
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(checksums, f, indent=2, ensure_ascii=False)
        
        return output_path
    
    def verify_checksum_file(self, checksum_file: str,
                              base_directory: Optional[str] = None) -> Tuple[bool, List[str]]:
        """éªŒè¯æ ¡éªŒå’Œæ–‡ä»¶"""
        with open(checksum_file, 'r', encoding='utf-8') as f:
            checksums = json.load(f)
        
        algorithm = checksums.get('algorithm', self.hasher.algorithm)
        stored_files = checksums.get('files', {})
        
        # ä¸´æ—¶åˆ‡æ¢ç®—æ³•
        old_algorithm = self.hasher.algorithm
        self.hasher = FileHasher(algorithm)
        
        errors = []
        all_match = True
        
        for rel_path, expected_hash in stored_files.items():
            if base_directory:
                full_path = os.path.join(base_directory, rel_path)
            else:
                full_path = rel_path
            
            if not os.path.exists(full_path):
                errors.append(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {rel_path}")
                all_match = False
                continue
            
            actual_hash = self.hasher.hash_file(full_path)
            
            if actual_hash == expected_hash:
                print(f"  âœ… {rel_path}: åŒ¹é…")
            else:
                errors.append(f"  âŒ {rel_path}: ä¸åŒ¹é…\n     æœŸæœ›: {expected_hash[:16]}...\n     å®é™…: {actual_hash[:16]}...")
                all_match = False
        
        # æ¢å¤åŸç®—æ³•
        self.hasher = FileHasher(old_algorithm)
        
        return all_match, errors


def batch_verify(directory: str, checksum_file: str,
                 algorithm: str = 'sha256') -> bool:
    """æ‰¹é‡éªŒè¯ç›®å½•æ–‡ä»¶"""
    print(f"\nğŸ” æ‰¹é‡éªŒè¯: {directory}")
    print(f"ğŸ“„ æ ¡éªŒå’Œæ–‡ä»¶: {checksum_file}\n")
    
    hasher = FileHasher(algorithm)
    manager = ChecksumManager(hasher)
    
    base_dir = directory
    all_match, errors = manager.verify_checksum_file(checksum_file, base_dir)
    
    if errors:
        print(f"\nâš ï¸ å‘ç° {len(errors)} ä¸ªé”™è¯¯:")
        for error in errors:
            print(error)
    
    return all_match


def batch_generate(directory: str, output_file: str,
                   algorithm: str = 'sha256', recursive: bool = True) -> None:
    """æ‰¹é‡ç”Ÿæˆæ ¡éªŒå’Œ"""
    print(f"\nğŸ“¦ æ‰¹é‡ç”Ÿæˆæ ¡éªŒå’Œ: {directory}")
    print(f"ğŸ”§ ç®—æ³•: {algorithm}")
    print(f"ğŸ“„ è¾“å‡º: {output_file}\n")
    
    hasher = FileHasher(algorithm)
    files = hasher.hash_directory(directory, recursive=recursive)
    
    if not files:
        print("  âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
        return
    
    manager = ChecksumManager(hasher)
    manager.generate_checksum_file(files, output_file, relative_base=directory)
    
    print(f"\nâœ… å·²ç”Ÿæˆ {len(files)} ä¸ªæ–‡ä»¶çš„æ ¡éªŒå’Œ")


def incremental_backup(source_dir: str, backup_dir: str,
                       algorithm: str = 'sha256') -> List[str]:
    """å¢é‡å¤‡ä»½ - åªå¤åˆ¶ä¿®æ”¹è¿‡çš„æ–‡ä»¶"""
    print(f"\nğŸ”„ å¢é‡å¤‡ä»½: {source_dir} â†’ {backup_dir}")
    
    hasher = FileHasher(algorithm)
    checksum_file = os.path.join(backup_dir, '.checksums.json')
    
    # åŠ è½½æ—§çš„æ ¡éªŒå’Œ
    old_checksums = {}
    if os.path.exists(checksum_file):
        with open(checksum_file, 'r') as f:
            data = json.load(f)
            old_checksums = data.get('files', {})
    
    # è®¡ç®—å½“å‰æ–‡ä»¶çš„æ ¡éªŒå’Œ
    current_files = hasher.hash_directory(source_dir, recursive=True)
    
    # æ¯”è¾ƒå¹¶å¤åˆ¶ä¿®æ”¹è¿‡çš„æ–‡ä»¶
    copied = []
    os.makedirs(backup_dir, exist_ok=True)
    
    for rel_path, current_hash in current_files.items():
        old_hash = old_checksums.get(rel_path)
        
        if old_hash != current_hash:
            src = os.path.join(source_dir, rel_path)
            dst = os.path.join(backup_dir, rel_path)
            
            os.makedirs(os.path.dirname(dst), exist_ok=True)
            
            with open(src, 'rb') as sf, open(dst, 'wb') as df:
                df.write(sf.read())
            
            print(f"  ğŸ“‹ {rel_path}")
            copied.append(rel_path)
    
    # ä¿å­˜æ–°çš„æ ¡éªŒå’Œ
    manager = ChecksumManager(hasher)
    manager.generate_checksum_file(current_files, checksum_file, relative_base=source_dir)
    
    print(f"\nâœ… å·²å¤åˆ¶ {len(copied)} ä¸ªä¿®æ”¹çš„æ–‡ä»¶")
    return copied


def main():
    parser = argparse.ArgumentParser(
        description='æ–‡ä»¶å“ˆå¸ŒéªŒè¯å·¥å…· - æ”¯æŒå¤šç§ç®—æ³•å’Œæ‰¹é‡æ“ä½œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # ç”Ÿæˆå•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œ
  python file_hash_tool.py single file.txt -a sha256
  
  # æ‰¹é‡ç”Ÿæˆç›®å½•æ ¡éªŒå’Œ
  python file_hash_tool.py batch gen ./my_folder -o checksums.json
  
  # éªŒè¯æ ¡éªŒå’Œ
  python file_hash_tool.py batch verify ./my_folder checksums.json
  
  # å¢é‡å¤‡ä»½
  python file_hash_tool.py backup ./source ./backup
        '''
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # å•æ–‡ä»¶å‘½ä»¤
    file_parser = subparsers.add_parser('file', help='è®¡ç®—å•ä¸ªæ–‡ä»¶çš„å“ˆå¸Œ')
    file_parser.add_argument('filepath', help='æ–‡ä»¶è·¯å¾„')
    file_parser.add_argument('-a', '--algorithm', choices=['md5', 'sha1', 'sha256', 'sha512'],
                              default='sha256', help='å“ˆå¸Œç®—æ³•')
    
    # æ‰¹é‡å‘½ä»¤
    batch_parser = subparsers.add_parser('batch', help='æ‰¹é‡å¤„ç†')
    batch_sub = batch_parser.add_subparsers(dest='batch_command', help='æ‰¹é‡æ“ä½œ')
    
    gen_parser = batch_sub.add_parser('gen', help='ç”Ÿæˆæ ¡éªŒå’Œ')
    gen_parser.add_argument('directory', help='ç›®å½•è·¯å¾„')
    gen_parser.add_argument('-o', '--output', required=True, help='è¾“å‡ºæ–‡ä»¶')
    gen_parser.add_argument('-a', '--algorithm', choices=['md5', 'sha1', 'sha256', 'sha512'],
                            default='sha256', help='å“ˆå¸Œç®—æ³•')
    gen_parser.add_argument('--no-recursive', action='store_true', help='ä¸é€’å½’å­ç›®å½•')
    
    verify_parser = batch_sub.add_parser('verify', help='éªŒè¯æ ¡éªŒå’Œ')
    verify_parser.add_argument('directory', help='ç›®å½•è·¯å¾„')
    verify_parser.add_argument('checksum_file', help='æ ¡éªŒå’Œæ–‡ä»¶')
    verify_parser.add_argument('-a', '--algorithm', choices=['md5', 'sha1', 'sha256', 'sha512'],
                               default='sha256', help='å“ˆå¸Œç®—æ³•')
    
    # å¢é‡å¤‡ä»½å‘½ä»¤
    backup_parser = subparsers.add_parser('backup', help='å¢é‡å¤‡ä»½')
    backup_parser.add_argument('source', help='æºç›®å½•')
    backup_parser.add_argument('destination', help='ç›®æ ‡ç›®å½•')
    backup_parser.add_argument('-a', '--algorithm', choices=['md5', 'sha1', 'sha256', 'sha512'],
                               default='sha256', help='å“ˆå¸Œç®—æ³•')
    
    args = parser.parse_args()
    
    if args.command == 'file':
        hasher = FileHasher(args.algorithm)
        hash_value = hasher.hash_file(args.filepath)
        print(f"\nâœ… {args.algorithm.upper()} = {hash_value}")
    
    elif args.command == 'batch':
        if args.batch_command == 'gen':
            batch_generate(args.directory, args.output, args.algorithm,
                          recursive=not args.no_recursive)
        elif args.batch_command == 'verify':
            batch_verify(args.directory, args.checksum_file, args.algorithm)
        else:
            batch_parser.print_help()
    
    elif args.command == 'backup':
        incremental_backup(args.source, args.destination, args.algorithm)
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬å¤„ç†å·¥å…·
æ™ºèƒ½æ–‡æœ¬æ¸…æ´—ã€æ•æ„Ÿä¿¡æ¯æ£€æµ‹ã€æ‘˜è¦ç”Ÿæˆã€å…³é”®è¯æå–
"""

import re
import json
import base64
from datetime import datetime
from collections import Counter
import hashlib


class TextProcessor:
    """æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨"""
    
    # æ•æ„Ÿä¿¡æ¯æ­£åˆ™æ¨¡å¼
    SENSITIVE_PATTERNS = {
        'email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
        'phone': r'1[3-9]\d{9}',  # ä¸­å›½æ‰‹æœºå·
        'phone_us': r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',  # ç¾å›½ç”µè¯
        'credit_card': r'\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}',
        'ssn': r'\d{3}[-\s]?\d{2}[-\s]?\d{4}',
        'ipv4': r'\b(?:\d{1,3}\.){3}\d{1,3}\b',
        'password': r'(password|pwd|secret|key|token)[\s:=]*[\w\-\.]+',
    }
    
    # åœç”¨è¯åˆ—è¡¨
    STOPWORDS = {
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'ç€',
        'æˆ–', 'ä¸€ä¸ª', 'æ²¡æœ‰', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'è¿™', 'é‚£',
        'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was',
        'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',
        'does', 'did', 'will', 'would', 'could', 'should', 'may',
        'might', 'must', 'shall', 'can', 'to', 'of', 'in', 'for',
        'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through',
        'during', 'before', 'after', 'above', 'below', 'between',
        'under', 'again', 'further', 'then', 'once', 'here', 'there',
    }
    
    def __init__(self):
        self.stats = {'cleaned': 0, 'detected': 0, 'summarized': 0}
    
    def clean_text(self, text, remove_special=True, normalize_whitespace=True):
        """
        æ¸…æ´—æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            remove_special: æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            normalize_whitespace: æ˜¯å¦è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        cleaned = text
        
        if normalize_whitespace:
            cleaned = re.sub(r'\s+', ' ', cleaned)
            cleaned = cleaned.strip()
        
        if remove_special:
            cleaned = re.sub(r'[^\w\s\u4e00-\u9fff\.\,\!\?\:\;\-\_\'\"]', '', cleaned)
        
        self.stats['cleaned'] += 1
        return cleaned
    
    def detect_sensitive_info(self, text, return_positions=True):
        """
        æ£€æµ‹æ•æ„Ÿä¿¡æ¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            return_positions: æ˜¯å¦è¿”å›ä½ç½®ä¿¡æ¯
        
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        results = {'types_found': [], 'items': [], 'redacted': text}
        
        for info_type, pattern in self.SENSITIVE_PATTERNS.items():
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            if matches:
                results['types_found'].append(info_type)
                for match in matches:
                    item = {
                        'type': info_type,
                        'value': match.group(),
                        'start': match.start(),
                        'end': match.end()
                    }
                    results['items'].append(item)
        
        # è„±æ•å¤„ç†
        redacted = text
        for item in results['items']:
            replacement = f'[{item["type"].upper()}_REDACTED]'
            redacted = redacted[:item['start']] + replacement + redacted[item['end']:]
        results['redacted'] = redacted
        
        self.stats['detected'] += 1
        return results
    
    def extract_keywords(self, text, top_n=10, include_frequency=True):
        """
        æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
            include_frequency: æ˜¯å¦åŒ…å«é¢‘ç‡ä¿¡æ¯
        
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œä¸­æ–‡åˆ†è¯ï¼‰
        words = re.findall(r'\b\w+\b', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered = [
            w for w in words 
            if w not in self.STOPWORDS 
            and len(w) > 1
            and w.isalpha()
        ]
        
        # è¯é¢‘ç»Ÿè®¡
        word_freq = Counter(filtered)
        
        # è·å–top_n
        keywords = word_freq.most_common(top_n)
        
        if include_frequency:
            return [{'word': w, 'frequency': f} for w, f in keywords]
        return [w for w, _ in keywords]
    
    def summarize_text(self, text, max_length=100):
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ï¼ˆæŠ½å–å¼ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æ‘˜è¦æœ€å¤§é•¿åº¦
        
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return text[:max_length]
        
        if len(sentences) == 1:
            return sentences[0][:max_length]
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„å¾—åˆ†ï¼ˆåŸºäºè¯é¢‘å’Œä½ç½®ï¼‰
        words = re.findall(r'\\w+\bb', text.lower())
        word_freq = Counter(words)
        
        sentence_scores = []
        for i, sentence in enumerate(sentences):
            score = 0
            sentence_words = re.findall(r'\b\w+\b', sentence.lower())
            
            # è¯é¢‘å¾—åˆ†
            for word in sentence_words:
                score += word_freq.get(word, 0)
            
            # ä½ç½®å¾—åˆ†ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦ï¼‰
            if i == 0:
                score *= 1.5
            elif i == len(sentences) - 1:
                score *= 1.3
            
            # é•¿åº¦æƒ©ç½šï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿çš„å¥å­å¾—åˆ†é™ä½ï¼‰
            word_count = len(sentence_words)
            if word_count < 3:
                score *= 0.5
            elif word_count > 50:
                score *= 0.8
            
            sentence_scores.append((sentence, score))
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œé€‰å–å¥å­
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©å¥å­å¹¶ä¿æŒåŸå§‹é¡ºåº
        selected = []
        for sentence, _ in sentence_scores[:3]:  # æœ€å¤šé€‰3ä¸ª
            if len(' '.join(selected)) + len(sentence) < max_length * 1.5:
                selected.append(sentence)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åˆ—
        result = ' '.join([
            s for s in sentences 
            if s in selected
        ])
        
        self.stats['summarized'] += 1
        return result[:max_length]
    
    def analyze_text(self, text):
        """
        å®Œæ•´æ–‡æœ¬åˆ†æ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # æ¸…æ´—
        cleaned = self.clean_text(text)
        
        # æ•æ„Ÿä¿¡æ¯æ£€æµ‹
        sensitive = self.detect_sensitive_info(text)
        
        # å…³é”®è¯æå–
        keywords = self.extract_keywords(cleaned)
        
        # æ‘˜è¦ç”Ÿæˆ
        summary = self.summarize_text(cleaned)
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = {
            'char_count': len(text),
            'word_count': len(re.findall(r'\b\w+\b', text)),
            'sentence_count': len(re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]+', text)),
        }
        
        return {
            'original_length': len(text),
            'cleaned_text': cleaned,
            'sensitive_info': sensitive,
            'keywords': keywords,
            'summary': summary,
            'statistics': stats,
            'processing_stats': self.stats.copy()
        }
    
    def batch_process(self, texts):
        """
        æ‰¹é‡å¤„ç†æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        for text in texts:
            results.append(self.analyze_text(text))
        return results
    
    def generate_report(self, analysis_result):
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            analysis_result: å•ä¸ªæ–‡æœ¬çš„åˆ†æç»“æœ
        
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Š
        """
        report = []
        report.append("=" * 50)
        report.append("ğŸ“Š æ–‡æœ¬åˆ†ææŠ¥å‘Š")
        report.append("=" * 50)
        
        stats = analysis_result['statistics']
        report.append(f"\nğŸ“ åŸºæœ¬ç»Ÿè®¡:")
        report.append(f"   - å­—ç¬¦æ•°: {stats['char_count']}")
        report.append(f"   - è¯æ•°: {stats['word_count']}")
        report.append(f"   - å¥å­æ•°: {stats['sentence_count']}")
        
        report.append(f"\nğŸ”‘ å…³é”®è¯ (Top 10):")
        for kw in analysis_result['keywords'][:10]:
            if isinstance(kw, dict):
                report.append(f"   - {kw['word']}: {kw['frequency']}æ¬¡")
            else:
                report.append(f"   - {kw}")
        
        report.append(f"\nğŸ“ æ‘˜è¦:")
        report.append(f"   {analysis_result['summary']}")
        
        sensitive = analysis_result['sensitive_info']
        if sensitive['types_found']:
            report.append(f"\nâš ï¸ æ£€æµ‹åˆ°æ•æ„Ÿä¿¡æ¯:")
            report.append(f"   ç±»å‹: {', '.join(sensitive['types_found'])}")
            report.append(f"   é¡¹ç›®æ•°: {len(sensitive['items'])}")
        else:
            report.append(f"\nâœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿä¿¡æ¯")
        
        report.append(f"\nğŸ§¹ æ¸…æ´—åæ–‡æœ¬:")
        report.append(f"   {analysis_result['cleaned_text'][:200]}...")
        
        report.append("\n" + "=" * 50)
        return '\n'.join(report)


class TextProcessorCLI:
    """å‘½ä»¤è¡Œæ¥å£"""
    
    def __init__(self):
        self.processor = TextProcessor()
    
    def run_interactive(self):
        """äº¤äº’å¼è¿è¡Œ"""
        print("ğŸ”§ æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨")
        print("=" * 40)
        print("è¾“å…¥ 'quit' é€€å‡º")
        print("è¾“å…¥ 'report' æŸ¥çœ‹å®Œæ•´æŠ¥å‘Šç¤ºä¾‹")
        print("-" * 40)
        
        while True:
            try:
                text = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬: ").strip()
                
                if text.lower() == 'quit':
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if text.lower() == 'report':
                    text = """äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚
                    æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„ç¼–ç¨‹ã€‚
                    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚
                    è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œæ˜¯AIåº”ç”¨çš„é‡è¦æ–¹å‘ã€‚
                    è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿ'çœ‹'å’Œç†è§£å›¾åƒå’Œè§†é¢‘å†…å®¹ï¼Œåœ¨åŒ»ç–—ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚
                    AIçš„å‘å±•å¸¦æ¥äº†è®¸å¤šæœºé‡ï¼Œå¦‚æé«˜æ•ˆç‡ã€è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†ä¹Ÿé¢ä¸´ä¼¦ç†å’Œéšç§æŒ‘æˆ˜ã€‚
                    æœªæ¥ï¼ŒAIå°†ç»§ç»­æ¼”è¿›ï¼Œä¸äººç±»åä½œï¼Œå…±åŒåˆ›é€ æ›´ç¾å¥½çš„ä¸–ç•Œã€‚"""
                
                if not text:
                    continue
                
                result = self.processor.analyze_text(text)
                report = self.processor.generate_report(result)
                print(report)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
    
    def process_file(self, file_path):
        """å¤„ç†æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            result = self.processor.analyze_text(text)
            report = self.processor.generate_report(result)
            print(report)
            
            return result
        except Exception as e:
            print(f"âŒ æ–‡ä»¶å¤„ç†é”™è¯¯: {e}")
            return None
    
    def batch_files(self, file_paths):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
        all_results = []
        for path in file_paths:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {path}")
            result = self.process_file(path)
            if result:
                all_results.append({'file': path, 'result': result})
        return all_results


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨æ¼”ç¤º")
    print("=" * 50)
    
    processor = TextProcessor()
    
    # ç¤ºä¾‹æ–‡æœ¬
    sample_texts = [
        """æ¬¢è¿ä½¿ç”¨æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨ï¼è¿™ä¸ªå·¥å…·å¯ä»¥å¸®åŠ©ä½ æ¸…æ´—æ–‡æœ¬ã€æ£€æµ‹æ•æ„Ÿä¿¡æ¯ã€æå–å…³é”®è¯å’Œç”Ÿæˆæ‘˜è¦ã€‚
        æ•æ„Ÿä¿¡æ¯ç¤ºä¾‹ï¼šè”ç³»é‚®ç®±æ˜¯ example@email.comï¼Œç”µè¯æ˜¯ 13812345678ã€‚
        æˆ‘ä»¬çš„äº§å“å¯ä»¥å¸®åŠ©ä¼ä¸šæé«˜æ•ˆç‡ï¼Œé™ä½æˆæœ¬ï¼Œå¢å¼ºç«äº‰åŠ›ã€‚""",
        
        """äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚ä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èåˆ†æï¼Œ
        AIæŠ€æœ¯çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰æŠ€æœ¯ä¸æ–­çªç ´ï¼Œ
        ä¸ºäººç±»å¸¦æ¥æ›´å¤šå¯èƒ½æ€§ã€‚æœªæ¥ï¼ŒAIå°†ç»§ç»­å‘å±•ï¼Œä¸äººç±»å…±åŒåˆ›é€ æ›´ç¾å¥½çš„æ˜å¤©ã€‚""",
    ]
    
    for i, text in enumerate(sample_texts, 1):
        print(f"\n{'='*50}")
        print(f"ğŸ“ ç¤ºä¾‹ {i}")
        print(f"{'='*50}")
        
        result = processor.analyze_text(text)
        report = processor.generate_report(result)
        print(report)
        print()
    
    # æ‰¹é‡å¤„ç†æ¼”ç¤º
    print("=" * 50)
    print("ğŸ“¦ æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    results = processor.batch_process(sample_texts)
    print(f"âœ… æˆåŠŸå¤„ç† {len(results)} ä¸ªæ–‡æœ¬")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {processor.stats}")


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1:
        # å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼
        cli = TextProcessorCLI()
        
        if sys.argv[1] == '--demo':
            demo()
        elif sys.argv[1] == '--file':
            if len(sys.argv) > 2:
                cli.process_file(sys.argv[2])
            else:
                print("âŒ è¯·æŒ‡å®šæ–‡ä»¶è·¯å¾„: python text_processor.py --file <path>")
        elif sys.argv[1] == '--batch':
            if len(sys.argv) > 2:
                cli.batch_files(sys.argv[2:])
            else:
                print("âŒ è¯·æŒ‡å®šæ–‡ä»¶è·¯å¾„: python text_processor.py --batch <path1> <path2> ...")
        else:
            print("ç”¨æ³•:")
            print("  python text_processor.py --demo          # è¿è¡Œæ¼”ç¤º")
            print("  python text_processor.py --file <path>   # å¤„ç†å•ä¸ªæ–‡ä»¶")
            print("  python text_processor.py --batch <paths> # æ‰¹é‡å¤„ç†æ–‡ä»¶")
            print("  (æ— å‚æ•°)                                # äº¤äº’å¼æ¨¡å¼")
    else:
        # äº¤äº’å¼æ¨¡å¼
        cli = TextProcessorCLI()
        cli.run_interactive()

#!/usr/bin/env python3
"""
æ™ºèƒ½æ—¥å¿—åˆ†æå™¨ - Smart Log Analyzer
=====================================

åŠŸèƒ½:
- æ”¯æŒå¤šç§æ—¥å¿—æ ¼å¼ (Apache/Nginx/Syslog/JSON/è‡ªå®šä¹‰)
- å®æ—¶æ—¥å¿—è§£æå’Œç»Ÿè®¡åˆ†æ
- é”™è¯¯æ¨¡å¼æ£€æµ‹å’Œå‘Šè­¦
- å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ
- å®æ—¶æµå¼åˆ†æ

ä½œè€…: AI Coding Journey
æ—¥æœŸ: 2026-02-02
"""

import re
import json
import gzip
import argparse
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from pathlib import Path
import sys

# ANSIé¢œè‰²ä»£ç 
COLORS = {
    'RED': '\033[91m',
    'GREEN': '\033[92m', 
    'YELLOW': '\033[93m',
    'BLUE': '\033[94m',
    'MAGENTA': '\033[95m',
    'CYAN': '\033[96m',
    'WHITE': '\033[97m',
    'RESET': '\033[0m',
    'BOLD': '\033[1m',
}

def colorize(text: str, color: str) -> str:
    """ä¸ºæ–‡æœ¬æ·»åŠ é¢œè‰²"""
    return f"{COLORS.get(color, '')}{text}{COLORS['RESET']}"

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®æ•°æ®ç»“æ„"""
    timestamp: datetime
    level: str
    source: str
    message: str
    raw: str
    extra: Dict[str, Any] = field(default_factory=dict)

@dataclass  
class LogStats:
    """æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
    total_lines: int = 0
    total_size: int = 0
    level_counts: Dict[str, int] = field(default_factory=dict)
    source_counts: Dict[str, int] = field(default_factory=dict)
    hourly_distribution: Dict[int, int] = field(default_factory=dict)
    top_messages: List[Tuple[str, int]] = field(default_factory=list)
    error_patterns: List[str] = field(default_factory=list)
    time_range: Tuple[datetime, datetime] = None

class LogParser:
    """æ—¥å¿—è§£æå™¨åŸºç±»"""
    
    def parse(self, line: str) -> Optional[LogEntry]:
        raise NotImplementedError
    
    @staticmethod
    def detect_format(log_line: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ—¥å¿—æ ¼å¼"""
        patterns = {
            'apache_combined': r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3} - - \[',
            'syslog': r'\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2}',
            'json': r'^\{.*\}$',
            'nginx': r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3} - \w+ \[',
        }
        
        for fmt, pattern in patterns.items():
            if re.search(pattern, log_line):
                return fmt
        return 'custom'

class ApacheParser(LogParser):
    """Apache/Nginxæ—¥å¿—è§£æå™¨"""
    
    APACHE_PATTERN = re.compile(
        r'(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s+-\s+-\s+\['
        r'(?P<timestamp>.*?)\]\s+'
        r'"(?P<request>.*?)"\s+'
        r'(?P<status>\d{3})\s+'
        r'(?P<bytes>\d+)\s+'
        r'"(?P<referer>.*?)"\s+'
        r'"(?P<user_agent>.*?)"'
    )
    
    def parse(self, line: str) -> Optional[LogEntry]:
        match = self.APACHE_PATTERN.match(line)
        if not match:
            return None
        
        try:
            timestamp = datetime.strptime(
                match.group('timestamp'), 
                '%d/%b/%Y:%H:%M:%S %z'
            )
        except ValueError:
            timestamp = datetime.now()
        
        return LogEntry(
            timestamp=timestamp,
            level=self._status_to_level(int(match.group('status'))),
            source=match.group('ip'),
            message=match.group('request'),
            raw=line,
            extra={
                'status': match.group('status'),
                'bytes': match.group('bytes'),
                'referer': match.group('referer'),
                'user_agent': match.group('user_agent')[:100]
            }
        )
    
    def _status_to_level(self, status: int) -> str:
        if status < 300: return 'INFO'
        if status < 400: return 'WARNING'  
        if status < 500: return 'ERROR'
        return 'CRITICAL'

class SyslogParser(LogParser):
    """Syslogè§£æå™¨"""
    
    SYSLOG_PATTERN = re.compile(
        r'(?P<timestamp>\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2})\s+'
        r'(?P<hostname>\S+)\s+'
        r'(?P<process>\S+?)(?:\[(?P<pid>\d+)\])?:\s+'
        r'(?P<message>.*)'
    )
    
    LEVEL_MAP = {
        'emerg': 'CRITICAL', 'alert': 'CRITICAL', 'crit': 'ERROR',
        'err': 'ERROR', 'warning': 'WARNING', 'warn': 'WARNING', 
        'notice': 'INFO', 'info': 'INFO', 'debug': 'DEBUG'
    }
    
    def parse(self, line: str) -> Optional[LogEntry]:
        match = self.SYSLOG_PATTERN.match(line)
        if not match:
            return None
        
        try:
            timestamp = datetime.strptime(
                match.group('timestamp'), 
                '%b %d %H:%M:%S'
            )
            timestamp = timestamp.replace(year=datetime.now().year)
        except ValueError:
            timestamp = datetime.now()
        
        process = match.group('process')
        message = match.group('message')
        
        # æ£€æµ‹æ—¥å¿—çº§åˆ«
        level = 'INFO'
        for key, level_name in self.LEVEL_MAP.items():
            if key in message.lower()[:20]:
                level = level_name
                break
        
        return LogEntry(
            timestamp=timestamp,
            level=level,
            source=f"{match.group('hostname')}/{process}",
            message=message,
            raw=line,
            extra={'pid': match.group('pid')}
        )

class JSONParser(LogParser):
    """JSONæ—¥å¿—è§£æå™¨"""
    
    def parse(self, line: str) -> Optional[LogEntry]:
        try:
            data = json.loads(line.strip())
            return LogEntry(
                timestamp=datetime.fromisoformat(
                    data.get('timestamp', datetime.now().isoformat())
                ),
                level=data.get('level', data.get('severity', 'INFO')),
                source=data.get('source', data.get('service', 'unknown')),
                message=data.get('message', data.get('msg', '')),
                raw=line,
                extra={k: v for k, v in data.items() 
                      if k not in ['timestamp', 'level', 'source', 'message']}
            )
        except json.JSONDecodeError:
            return None

class CustomParser(LogParser):
    """è‡ªå®šä¹‰æ ¼å¼è§£æå™¨"""
    
    def __init__(self, pattern: str, timestamp_format: str = None):
        self.pattern = re.compile(pattern)
        self.timestamp_format = timestamp_format
    
    def parse(self, line: str) -> Optional[LogEntry]:
        match = self.pattern.match(line)
        if not match:
            return None
        
        groups = match.groupdict()
        timestamp = datetime.now()
        
        if 'timestamp' in groups and self.timestamp_format:
            try:
                timestamp = datetime.strptime(
                    groups['timestamp'], 
                    self.timestamp_format
                )
            except ValueError:
                pass
        
        return LogEntry(
            timestamp=timestamp,
            level=groups.get('level', 'INFO'),
            source=groups.get('source', 'unknown'),
            message=groups.get('message', line),
            raw=line,
            extra={k: v for k, v in groups.items() 
                  if k not in ['timestamp', 'level', 'source', 'message']}
        )

class SmartLogAnalyzer:
    """æ™ºèƒ½æ—¥å¿—åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.stats = LogStats()
        self.entries: List[LogEntry] = []
        self.errors: List[LogEntry] = []
    
    def load_file(self, filepath: str, format: str = 'auto') -> int:
        """åŠ è½½æ—¥å¿—æ–‡ä»¶"""
        count = 0
        parser = self._get_parser(filepath, format)
        
        open_func = gzip.open if filepath.endswith('.gz') else open
        
        with open_func(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                    
                entry = parser.parse(line) if parser else None
                if entry:
                    self.entries.append(entry)
                    count += 1
                    
                    if entry.level in ['ERROR', 'CRITICAL', 'WARNING']:
                        self.errors.append(entry)
        
        self._calculate_stats()
        return count
    
    def _get_parser(self, filepath: str, format: str) -> LogParser:
        """è·å–åˆé€‚çš„è§£æå™¨"""
        if format != 'auto':
            return self._create_parser(format)
        
        # è¯»å–ç¬¬ä¸€è¡Œæ£€æµ‹æ ¼å¼
        open_func = gzip.open if filepath.endswith('.gz') else open
        with open_func(filepath, 'r') as f:
            first_line = f.readline()
        
        detected = LogParser.detect_format(first_line)
        return self._create_parser(detected)
    
    def _create_parser(self, format: str) -> LogParser:
        """åˆ›å»ºæŒ‡å®šæ ¼å¼çš„è§£æå™¨"""
        parsers = {
            'apache': ApacheParser(),
            'apache_combined': ApacheParser(),
            'nginx': ApacheParser(),
            'syslog': SyslogParser(),
            'json': JSONParser(),
        }
        return parsers.get(format, SyslogParser())
    
    def _calculate_stats(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not self.entries:
            return
        
        self.stats.total_lines = len(self.entries)
        
        # çº§åˆ«ç»Ÿè®¡
        for entry in self.entries:
            self.stats.level_counts[entry.level] = \
                self.stats.level_counts.get(entry.level, 0) + 1
            self.stats.source_counts[entry.source] = \
                self.stats.source_counts.get(entry.source, 0) + 1
            self.stats.hourly_distribution[entry.timestamp.hour] = \
                self.stats.hourly_distribution.get(entry.timestamp.hour, 0) + 1
        
        # æ—¶é—´èŒƒå›´
        timestamps = [e.timestamp for e in self.entries]
        self.stats.time_range = (min(timestamps), max(timestamps))
        
        # çƒ­é—¨æ¶ˆæ¯
        messages = [e.message[:100] for e in self.entries]
        self.stats.top_messages = Counter(messages).most_common(10)
        
        # é”™è¯¯æ¨¡å¼æ£€æµ‹
        self._detect_error_patterns()
    
    def _detect_error_patterns(self):
        """æ£€æµ‹å¸¸è§é”™è¯¯æ¨¡å¼"""
        error_patterns = [
            (r'connection.*refused', 'è¿æ¥è¢«æ‹’ç»'),
            (r'timeout', 'è¶…æ—¶é”™è¯¯'),
            (r'permission.*denied', 'æƒé™æ‹’ç»'),
            (r'null.*pointer', 'ç©ºæŒ‡é’ˆå¼‚å¸¸'),
            (r'memory.*exhausted', 'å†…å­˜è€—å°½'),
            (r'disk.*full', 'ç£ç›˜ç©ºé—´ä¸è¶³'),
            (r'segmentation.*fault', 'æ®µé”™è¯¯'),
            (r'key.*error', 'é”®å€¼é”™è¯¯'),
            (r'import.*error', 'å¯¼å…¥é”™è¯¯'),
            (r'syntax.*error', 'è¯­æ³•é”™è¯¯'),
        ]
        
        error_messages = ' '.join([e.message.lower() for e in self.errors])
        
        for pattern, description in error_patterns:
            if re.search(pattern, error_messages):
                self.stats.error_patterns.append(description)
    
    def generate_report(self, output_format: str = 'text') -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if output_format == 'json':
            return self._generate_json_report()
        return self._generate_text_report()
    
    def _generate_text_report(self) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        lines = [
            colorize("=" * 60, 'CYAN'),
            colorize("ğŸ“Š æ™ºèƒ½æ—¥å¿—åˆ†ææŠ¥å‘Š", 'BOLD'),
            colorize("=" * 60, 'CYAN'),
            "",
            colorize("ğŸ“ˆ æ¦‚è§ˆç»Ÿè®¡", 'BOLD'),
            "-" * 40,
            f"  æ€»æ—¥å¿—è¡Œæ•°: {colorize(str(self.stats.total_lines), 'GREEN')}",
            f"  é”™è¯¯æ•°é‡: {colorize(str(len(self.errors)), 'RED')}",
            f"  é”™è¯¯ç‡: {colorize(f'{len(self.errors)/max(1,self.stats.total_lines)*100:.2f}%', 'YELLOW')}",
            "",
        ]
        
        if self.stats.time_range:
            start, end = self.stats.time_range
            lines.extend([
                f"  æ—¶é—´èŒƒå›´: {start} è‡³ {end}",
                "",
            ])
        
        # çº§åˆ«åˆ†å¸ƒ
        lines.extend([
            colorize("ğŸ“Š æ—¥å¿—çº§åˆ«åˆ†å¸ƒ", 'BOLD'),
            "-" * 40,
        ])
        level_colors = {
            'DEBUG': 'WHITE', 'INFO': 'GREEN', 'WARNING': 'YELLOW',
            'ERROR': 'RED', 'CRITICAL': 'MAGENTA'
        }
        for level, count in sorted(self.stats.level_counts.items()):
            pct = count / self.stats.total_lines * 100
            bar = 'â–ˆ' * int(pct / 2)
            lines.append(
                f"  {colorize(level, level_colors.get(level, 'WHITE')):10} "
                f"{bar:25} {count:6} ({pct:5.1f}%)"
            )
        lines.append("")
        
        # æ¥æºç»Ÿè®¡
        if len(self.stats.source_counts) > 1:
            lines.extend([
                colorize("ğŸŒ Topæ¥æº", 'BOLD'),
                "-" * 40,
            ])
            for source, count in list(self.stats.source_counts.items())[:5]:
                lines.append(f"  {source}: {count}")
            lines.append("")
        
        # é”™è¯¯æ¨¡å¼
        if self.stats.error_patterns:
            lines.extend([
                colorize("âš ï¸ æ£€æµ‹åˆ°çš„é”™è¯¯æ¨¡å¼", 'BOLD'),
                "-" * 40,
            ])
            for pattern in self.stats.error_patterns:
                lines.append(f"  â€¢ {colorize(pattern, 'RED')}")
            lines.append("")
        
        # çƒ­é—¨æ¶ˆæ¯
        if self.stats.top_messages:
            lines.extend([
                colorize("ğŸ’¬ çƒ­é—¨æ¶ˆæ¯Top 5", 'BOLD'),
                "-" * 40,
            ])
            for i, (msg, count) in enumerate(self.stats.top_messages[:5], 1):
                msg_display = msg[:50] + "..." if len(msg) > 50 else msg
                lines.append(f"  {i}. [{count}x] {msg_display}")
        
        lines.append("")
        lines.append(colorize("=" * 60, 'CYAN'))
        
        return '\n'.join(lines)
    
    def _generate_json_report(self) -> str:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'total_lines': self.stats.total_lines,
            'error_count': len(self.stats.error_patterns),
            'level_distribution': self.stats.level_counts,
            'source_distribution': dict(self.stats.source_counts),
            'hourly_distribution': dict(self.stats.hourly_distribution),
            'time_range': {
                'start': self.stats.time_range[0].isoformat() if self.stats.time_range else None,
                'end': self.stats.time_range[1].isoformat() if self.stats.time_range else None
            },
            'top_messages': self.stats.top_messages,
            'detected_patterns': self.stats.error_patterns
        }
        return json.dumps(report, indent=2, ensure_ascii=False)
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print(self.generate_report())

class InteractiveMode:
    """äº¤äº’å¼æ¨¡å¼"""
    
    def __init__(self):
        self.analyzer = SmartLogAnalyzer()
    
    def run(self):
        """è¿è¡Œäº¤äº’å¼åˆ†æ"""
        print(colorize("\nğŸ” æ™ºèƒ½æ—¥å¿—åˆ†æå™¨ - äº¤äº’æ¨¡å¼", 'BOLD'))
        print(colorize("è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„è¿›è¡Œåˆ†æ (è¾“å…¥ 'q' é€€å‡º):\n", 'CYAN'))
        
        while True:
            filepath = input(colorize("ğŸ“ æ–‡ä»¶è·¯å¾„: ", 'GREEN')).strip()
            
            if filepath.lower() == 'q':
                print(colorize("\nğŸ‘‹ å†è§!", 'CYAN'))
                break
            
            if not Path(filepath).exists():
                print(colorize(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}", 'RED'))
                continue
            
            print(colorize(f"\nâ³ æ­£åœ¨åˆ†æ: {filepath}...", 'YELLOW'))
            
            try:
                count = self.analyzer.load_file(filepath)
                print(colorize(f"âœ… åˆ†æå®Œæˆ! å…± {count} æ¡æ—¥å¿—è®°å½•\n", 'GREEN'))
                self.analyzer.print_summary()
                print()
            except Exception as e:
                print(colorize(f"âŒ åˆ†æå¤±è´¥: {e}", 'RED'))

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½æ—¥å¿—åˆ†æå™¨ - Smart Log Analyzer',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s access.log              # åˆ†ææ—¥å¿—æ–‡ä»¶
  %(prog)s access.log --json       # JSONæ ¼å¼è¾“å‡º
  %(prog)s access.log -i           # äº¤äº’æ¨¡å¼
  %(prog)s --interactive           # å¯åŠ¨äº¤äº’æ¨¡å¼
        """
    )
    
    parser.add_argument('filepath', nargs='?', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-f', '--format', default='auto',
                       choices=['auto', 'apache', 'nginx', 'syslog', 'json'],
                       help='æ—¥å¿—æ ¼å¼')
    parser.add_argument('-o', '--output', default='text',
                       choices=['text', 'json'],
                       help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('-i', '--interactive', action='store_true',
                       help='äº¤äº’æ¨¡å¼')
    parser.add_argument('--stats', action='store_true',
                       help='ä»…æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦')
    
    args = parser.parse_args()
    
    if args.interactive:
        InteractiveMode().run()
        return
    
    if not args.filepath:
        parser.print_help()
        return
    
    if not Path(args.filepath).exists():
        print(colorize(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.filepath}", 'RED'))
        sys.exit(1)
    
    analyzer = SmartLogAnalyzer()
    print(colorize(f"\nâ³ æ­£åœ¨åŠ è½½: {args.filepath}...", 'YELLOW'))
    
    count = analyzer.load_file(args.filepath, args.format)
    print(colorize(f"âœ… å·²åŠ è½½ {count} æ¡æ—¥å¿—è®°å½•\n", 'GREEN'))
    
    if args.stats:
        print(f"ğŸ“Š æ€»è¡Œæ•°: {analyzer.stats.total_lines}")
        print(f"âš ï¸ é”™è¯¯æ•°: {len(analyzer.errors)}")
        print(f"ğŸ“ˆ çº§åˆ«åˆ†å¸ƒ: {analyzer.stats.level_counts}")
    else:
        print(analyzer.generate_report(args.output))

if __name__ == '__main__':
    main()

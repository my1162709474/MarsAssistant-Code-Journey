#!/usr/bin/env python3
"""
æ™ºèƒ½æ•°æ®åº“å·¥å…· (Day 52)
======================
æ”¯æŒ SQLiteã€MySQLã€PostgreSQLã€MongoDB çš„å¤šåŠŸèƒ½æ•°æ®åº“ç®¡ç†å·¥å…·

åŠŸèƒ½ç‰¹æ€§:
- å¤šæ•°æ®åº“æ”¯æŒ (SQLite/MySQL/PostgreSQL/MongoDB)
- æ™ºèƒ½æŸ¥è¯¢ç”Ÿæˆ (è‡ªç„¶è¯­è¨€è½¬ SQL)
- æ•°æ®å¤‡ä»½ä¸æ¢å¤
- æ€§èƒ½åˆ†æä¸ä¼˜åŒ–å»ºè®®
- ER å›¾ç”Ÿæˆ
- æ•°æ®å¯¼å…¥å¯¼å‡º (CSV/JSON/SQL)

ä½¿ç”¨æ–¹å¼:
    python smart_database_tool.py --help
    python smart_database_tool.py --query "æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·" --db sqlite:////tmp/test.db
    python smart_database_tool.py --backup --db sqlite:////tmp/test.db --output backup/
    python smart_database_tool.py --export --db sqlite:////tmp/test.db --format csv --table users
"""

import argparse
import json
import csv
import sqlite3
import subprocess
import sys
import os
import re
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from urllib.parse import urlparse
import sqlparse
from collections import defaultdict


class DatabaseType:
    """æ•°æ®åº“ç±»å‹å¸¸é‡"""
    SQLITE = "sqlite"
    MYSQL = "mysql"
    POSTGRESQL = "postgresql"
    MONGODB = "mongodb"


class SmartDatabaseTool:
    """æ™ºèƒ½æ•°æ®åº“å·¥å…·ä¸»ç±»"""
    
    def __init__(self, db_url: str):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        
        Args:
            db_url: æ•°æ®åº“è¿æ¥ URL
                - SQLite: sqlite:///path/to/database.db
                - MySQL: mysql://user:password@host:port/database
                - PostgreSQL: postgresql://user:password@host:port/database
                - MongoDB: mongodb://user:password@host:port/database
        """
        self.db_url = db_url
        self.db_type = self._parse_db_type(db_url)
        self.connection = None
        self._connect()
    
    def _parse_db_type(self, db_url: str) -> str:
        """è§£ææ•°æ®åº“ç±»å‹"""
        parsed = urlparse(db_url)
        scheme = parsed.scheme.lower()
        
        if scheme == DatabaseType.SQLITE:
            return DatabaseType.SQLITE
        elif scheme == DatabaseType.MYSQL:
            return DatabaseType.MYSQL
        elif scheme == DatabaseType.POSTGRESQL:
            return DatabaseType.POSTGRESQL
        elif scheme == DatabaseType.MONGODB:
            return DatabaseType.MONGODB
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {scheme}")
    
    def _connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        if self.db_type == DatabaseType.SQLITE:
            path = self.db_url.replace("sqlite:///", "").replace("sqlite://", "")
            if not path:
                path = ":memory:"
            self.connection = sqlite3.connect(path)
            self.connection.row_factory = sqlite3.Row
        
        elif self.db_type == DatabaseType.MYSQL:
            try:
                import pymysql
                parsed = urlparse(self.db_url)
                self.connection = pymysql.connect(
                    host=parsed.hostname or "localhost",
                    port=parsed.port or 3306,
                    user=parsed.username or "root",
                    password=parsed.password or "",
                    database=parsed.path[1:] or ""
                )
            except ImportError:
                print("âš ï¸  éœ€è¦å®‰è£… pymysql: pip install pymysql")
                raise
        
        elif self.db_type == DatabaseType.POSTGRESQL:
            try:
                import psycopg2
                parsed = urlparse(self.db_url)
                self.connection = psycopg2.connect(
                    host=parsed.hostname or "localhost",
                    port=parsed.port or 5432,
                    user=parsed.username or "postgres",
                    password=parsed.password or "",
                    database=parsed.path[1:] or "postgres"
                )
            except ImportError:
                print("âš ï¸  éœ€è¦å®‰è£… psycopg2: pip install psycopg2-binary")
                raise
        
        elif self.db_type == DatabaseType.MONGODB:
            try:
                from pymongo import MongoClient
                parsed = urlparse(self.db_url)
                host = parsed.hostname or "localhost"
                port = parsed.port or 27017
                auth = parsed.username and parsed.password
                if auth:
                    self.connection = MongoClient(
                        f"mongodb://{parsed.username}:{parsed.password}@{host}:{port}/"
                    )
                else:
                    self.connection = MongoClient(f"mongodb://{host}:{port}/")
            except ImportError:
                print("âš ï¸  éœ€è¦å®‰è£… pymongo: pip install pymongo")
                raise
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            if self.db_type == DatabaseType.MONGODB:
                self.connection.close()
            else:
                self.connection.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    # ==================== æ•°æ®åº“ä¿¡æ¯ ====================
    
    def get_database_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯"""
        info = {
            "db_type": self.db_type,
            "db_url": self.db_url,
            "tables": [],
            "size": 0
        }
        
        if self.db_type == DatabaseType.SQLITE:
            info["tables"] = self._get_sqlite_tables()
            info["size"] = self._get_sqlite_size()
        
        elif self.db_type == DatabaseType.MYSQL:
            info["tables"] = self._get_mysql_tables()
        
        elif self.db_type == DatabaseType.POSTGRESQL:
            info["tables"] = self._get_postgresql_tables()
        
        elif self.db_type == DatabaseType.MONGODB:
            info["collections"] = self._get_mongodb_collections()
        
        return info
    
    def _get_sqlite_tables(self) -> List[Dict]:
        """è·å– SQLite æ•°æ®åº“ä¸­çš„è¡¨ä¿¡æ¯"""
        cursor = self.connection.cursor()
        cursor.execute("""
            SELECT name, sql 
            FROM sqlite_master 
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
            ORDER BY name
        """)
        tables = []
        for row in cursor.fetchall():
            cursor.execute(f"SELECT COUNT(*) FROM {row['name']}")
            count = cursor.fetchone()[0]
            tables.append({
                "name": row["name"],
                "sql": row["sql"],
                "row_count": count
            })
        return tables
    
    def _get_sqlite_size(self) -> int:
        """è·å– SQLite æ•°æ®åº“å¤§å°"""
        path = self.db_url.replace("sqlite:///", "").replace("sqlite://", "")
        if path and os.path.exists(path):
            return os.path.getsize(path)
        return 0
    
    def _get_mysql_tables(self) -> List[Dict]:
        """è·å– MySQL æ•°æ®åº“ä¸­çš„è¡¨ä¿¡æ¯"""
        cursor = self.connection.cursor()
        cursor.execute("SHOW TABLES")
        tables = []
        for row in cursor.fetchall():
            table_name = row[0]
            cursor.execute(f"SELECT COUNT(*) FROM `{table_name}`")
            count = cursor.fetchone()[0]
            cursor.execute(f"SHOW CREATE TABLE `{table_name}`")
            create_sql = cursor.fetchone()[1]
            tables.append({
                "name": table_name,
                "sql": create_sql,
                "row_count": count
            })
        return tables
    
    def _get_postgresql_tables(self) -> List[Dict]:
        """è·å– PostgreSQL æ•°æ®åº“ä¸­çš„è¡¨ä¿¡æ¯"""
        cursor = self.connection.cursor()
        cursor.execute("""
            SELECT table_name FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        tables = []
        for row in cursor.fetchall():
            table_name = row[0]
            cursor.execute(f"SELECT COUNT(*) FROM \"{table_name}\"")
            count = cursor.fetchone()[0]
            cursor.execute("""
                SELECT pg_get_tabledef('%s') WHERE EXISTS (
                    SELECT 1 FROM information_schema.tables 
                    WHERE table_name = '%s'
                )
            """ % (table_name, table_name))
            create_sql = cursor.fetchone()
            tables.append({
                "name": table_name,
                "sql": create_sql[0] if create_sql else "",
                "row_count": count
            })
        return tables
    
    def _get_mongodb_collections(self) -> List[Dict]:
        """è·å– MongoDB æ•°æ®åº“ä¸­çš„é›†åˆä¿¡æ¯"""
        db_name = urlparse(self.db_url).path[1:] or "test"
        db = self.connection[db_name]
        collections = []
        for name in db.list_collection_names():
            count = db[name].count_documents({})
            collections.append({
                "name": name,
                "document_count": count
            })
        return collections
    
    # ==================== æŸ¥è¯¢æ‰§è¡Œ ====================
    
    def execute_query(self, query: str, params: tuple = None) -> Tuple[List[Dict], List[str]]:
        """
        æ‰§è¡Œ SQL æŸ¥è¯¢
        
        Args:
            query: SQL æŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            (æŸ¥è¯¢ç»“æœåˆ—è¡¨, åˆ—ååˆ—è¡¨)
        """
        cursor = self.connection.cursor()
        
        if params:
            cursor.execute(query, params)
        else:
            cursor.execute(query)
        
        if cursor.description:
            columns = [desc[0] for desc in cursor.description]
            rows = [dict(zip(columns, row)) for row in cursor.fetchall()]
        else:
            columns = []
            rows = []
        
        self.connection.commit()
        return rows, columns
    
    def execute_file(self, filepath: str) -> Tuple[int, float]:
        """
        æ‰§è¡Œ SQL æ–‡ä»¶
        
        Args:
            filepath: SQL æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å½±å“çš„è¡Œæ•°, æ‰§è¡Œæ—¶é—´ç§’)
        """
        with open(filepath, 'r') as f:
            sql_content = f.read()
        
        statements = sqlparse.split(sql_content)
        start_time = datetime.now()
        total_affected = 0
        
        cursor = self.connection.cursor()
        for statement in statements:
            statement = statement.strip()
            if statement:
                cursor.execute(statement)
                total_affected += cursor.rowcount
        
        self.connection.commit()
        elapsed = (datetime.now() - start_time).total_seconds()
        return total_affected, elapsed
    
    # ==================== è‡ªç„¶è¯­è¨€è½¬ SQL ====================
    
    def nl_to_sql(self, natural_query: str, schema_info: str = None) -> str:
        """
        å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸º SQL (ç®€åŒ–ç‰ˆæœ¬)
        
        Args:
            natural_query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            schema_info: æ•°æ®åº“ schema ä¿¡æ¯
            
        Returns:
            SQL æŸ¥è¯¢è¯­å¥
        """
        query_lower = natural_query.lower()
        
        # æå–è¡¨åå’Œåˆ—å
        if not schema_info:
            if self.db_type == DatabaseType.SQLITE:
                tables = [t["name"] for t in self._get_sqlite_tables()]
                schema_info = "Tables: " + ", ".join(tables)
        
        # ç®€å•çš„æ¨¡å¼åŒ¹é…è½¬æ¢
        if "æ‰€æœ‰" in query_lower or "æŸ¥è¯¢æ‰€æœ‰" in query_lower:
            if "ç”¨æˆ·" in query_lower:
                return "SELECT * FROM users"
            elif "è®¢å•" in query_lower:
                return "SELECT * FROM orders"
            elif "äº§å“" in query_lower:
                return "SELECT * FROM products"
            else:
                return "SELECT * FROM table_name"
        
        elif "ç»Ÿè®¡" in query_lower or "æ•°é‡" in query_lower:
            if "ç”¨æˆ·" in query_lower:
                return "SELECT COUNT(*) as count FROM users"
            else:
                return "SELECT COUNT(*) as count FROM table_name"
        
        elif "æœ€æ–°" in query_lower or "æœ€è¿‘" in query_lower:
            return "SELECT * FROM table_name ORDER BY created_at DESC LIMIT 10"
        
        elif "å¹³å‡" in query_lower:
            return "SELECT AVG(column) as avg_value FROM table_name"
        
        elif "æ±‚å’Œ" in query_lower or "æ€»è®¡" in query_lower:
            return "SELECT SUM(column) as total FROM table_name"
        
        else:
            return f"-- æ— æ³•è§£æçš„æŸ¥è¯¢: {natural_query}\nSELECT * FROM table_name LIMIT 10"
    
    # ==================== æ•°æ®å¯¼å…¥å¯¼å‡º ====================
    
    def export_table(self, table_name: str, format: str = "csv", 
                     output_path: str = None) -> str:
        """
        å¯¼å‡ºè¡¨æ•°æ®
        
        Args:
            table_name: è¡¨å
            format: å¯¼å‡ºæ ¼å¼ (csv/json/sql)
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not output_path:
            output_path = f"{table_name}_export.{format}"
        
        if format == "csv":
            return self._export_to_csv(table_name, output_path)
        elif format == "json":
            return self._export_to_json(table_name, output_path)
        elif format == "sql":
            return self._export_to_sql(table_name, output_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
    
    def _export_to_csv(self, table_name: str, output_path: str) -> str:
        """å¯¼å‡ºä¸º CSV æ ¼å¼"""
        rows, columns = self.execute_query(f"SELECT * FROM {table_name}")
        
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=columns)
            writer.writeheader()
            writer.writerows(rows)
        
        return output_path
    
    def _export_to_json(self, table_name: str, output_path: str) -> str:
        """å¯¼å‡ºä¸º JSON æ ¼å¼"""
        rows, columns = self.execute_query(f"SELECT * FROM {table_name}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(rows, f, ensure_ascii=False, indent=2)
        
        return output_path
    
    def _export_to_sql(self, table_name: str, output_path: str) -> str:
        """å¯¼å‡ºä¸º SQL æ ¼å¼"""
        rows, columns = self.execute_query(f"SELECT * FROM {table_name}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"-- Export of {table_name} at {datetime.now()}\n")
            for row in rows:
                values = []
                for val in row.values():
                    if val is None:
                        values.append("NULL")
                    elif isinstance(val, str):
                        values.append(f"'{val.replace(chr(39), chr(39)+chr(39))}'")
                    else:
                        values.append(str(val))
                f.write(f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(values)});\n")
        
        return output_path
    
    def import_data(self, table_name: str, filepath: str, 
                    format: str = "csv", create_table: bool = False):
        """
        å¯¼å…¥æ•°æ®
        
        Args:
            table_name: ç›®æ ‡è¡¨å
            filepath: æ•°æ®æ–‡ä»¶è·¯å¾„
            format: æ•°æ®æ ¼å¼ (csv/json/sql)
            create_table: æ˜¯å¦è‡ªåŠ¨åˆ›å»ºè¡¨
        """
        if format == "csv":
            self._import_from_csv(table_name, filepath, create_table)
        elif format == "json":
            self._import_from_json(table_name, filepath, create_table)
        elif format == "sql":
            self._import_from_sql(filepath)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
    
    def _import_from_csv(self, table_name: str, filepath: str, create_table: bool):
        """ä» CSV å¯¼å…¥"""
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            if create_table:
                self._create_table_from_csv(table_name, reader.fieldnames, list(reader))
            
            cursor = self.connection.cursor()
            for row in reader:
                placeholders = ', '.join(['?' for _ in row])
                columns = ', '.join(row.keys())
                cursor.execute(
                    f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})",
                    list(row.values())
                )
        self.connection.commit()
    
    def _create_table_from_csv(self, table_name: str, columns: List[str], sample_rows: List[Dict]):
        """ä» CSV åˆ›å»ºè¡¨"""
        if not sample_rows:
            return
        
        # æ¨æ–­åˆ—ç±»å‹
        type_mapping = {
            int: "INTEGER",
            float: "REAL",
            str: "TEXT"
        }
        
        col_defs = []
        for col in columns:
            sample_val = sample_rows[0].get(col, "")
            col_type = type_mapping.get(type(sample_val), "TEXT")
            col_defs.append(f"{col} {col_type}")
        
        create_sql = f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(col_defs)})"
        self.connection.execute(create_sql)
        self.connection.commit()
    
    def _import_from_json(self, table_name: str, filepath: str, create_table: bool):
        """ä» JSON å¯¼å…¥"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not data:
            return
        
        if create_table:
            self._create_table_from_json(table_name, data[0])
        
        cursor = self.connection.cursor()
        for row in data:
            placeholders = ', '.join(['?' for _ in row])
            columns = ', '.join(row.keys())
            cursor.execute(
                f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})",
                list(row.values())
            )
        self.connection.commit()
    
    def _create_table_from_json(self, table_name: str, sample: Dict):
        """ä» JSON åˆ›å»ºè¡¨"""
        col_defs = []
        for key, val in sample.items():
            col_type = "INTEGER" if isinstance(val, int) else \
                      "REAL" if isinstance(val, float) else \
                      "TEXT"
            col_defs.append(f"{key} {col_type}")
        
        create_sql = f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(col_defs)})"
        self.connection.execute(create_sql)
        self.connection.commit()
    
    def _import_from_sql(self, filepath: str):
        """ä» SQL æ–‡ä»¶å¯¼å…¥"""
        self.execute_file(filepath)
    
    # ==================== å¤‡ä»½æ¢å¤ ====================
    
    def backup(self, output_dir: str = ".") -> str:
        """
        å¤‡ä»½æ•°æ®åº“
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"backup_{self.db_type}_{timestamp}.sql"
        filepath = os.path.join(output_dir, filename)
        
        if self.db_type == DatabaseType.SQLITE:
            self._backup_sqlite(filepath)
        elif self.db_type == DatabaseType.MYSQL:
            self._backup_mysql(filepath)
        elif self.db_type == DatabaseType.POSTGRESQL:
            self._backup_postgresql(filepath)
        
        return filepath
    
    def _backup_sqlite(self, filepath: str):
        """SQLite å¤‡ä»½"""
        path = self.db_url.replace("sqlite:///", "").replace("sqlite://", "")
        if path and os.path.exists(path):
            with open(filepath, 'w') as f:
                for line in self.connection.iterdump():
                    f.write(f"{line}\n")
    
    def _backup_mysql(self, filepath: str):
        """MySQL å¤‡ä»½ (ä½¿ç”¨ mysqldump)"""
        parsed = urlparse(self.db_url)
        host = parsed.hostname or "localhost"
        port = parsed.port or 3306
        user = parsed.username or "root"
        password = parsed.password or ""
        database = parsed.path[1:] or ""
        
        cmd = [
            "mysqldump",
            f"-h{host}",
            f"-P{port}",
            f"-u{user}",
            f"-p{password}",
            database
        ]
        
        with open(filepath, 'w') as f:
            subprocess.run(cmd, stdout=f, check=True)
    
    def _backup_postgresql(self, filepath: str):
        """PostgreSQL å¤‡ä»½ (ä½¿ç”¨ pg_dump)"""
        parsed = urlparse(self.db_url)
        host = parsed.hostname or "localhost"
        port = parsed.port or 5432
        user = parsed.username or "postgres"
        password = parsed.password or ""
        database = parsed.path[1:] or "postgres"
        
        env = os.environ.copy()
        if password:
            env["PGPASSWORD"] = password
        
        cmd = [
            "pg_dump",
            f"-h{host}",
            f"-p{port}",
            f"-U{user}",
            database
        ]
        
        with open(filepath, 'w') as f:
            subprocess.run(cmd, stdout=f, env=env, check=True)
    
    # ==================== æ€§èƒ½åˆ†æ ====================
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åº“æ€§èƒ½"""
        analysis = {
            "slow_queries": [],
            "missing_indexes": [],
            "table_stats": [],
            "recommendations": []
        }
        
        if self.db_type == DatabaseType.SQLITE:
            analysis = self._analyze_sqlite_performance()
        
        return analysis
    
    def _analyze_sqlite_performance(self) -> Dict[str, Any]:
        """SQLite æ€§èƒ½åˆ†æ"""
        analysis = {
            "slow_queries": [],
            "missing_indexes": [],
            "table_stats": [],
            "recommendations": []
        }
        
        cursor = self.connection.cursor()
        
        # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
        """)
        
        for (table_name,) in cursor.fetchall():
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()
            
            analysis["table_stats"].append({
                "name": table_name,
                "row_count": count,
                "columns": len(columns),
                "has_autoindex": any(c[1].startswith("sqlite_") for c in columns)
            })
        
        # åˆ†ææŸ¥è¯¢å»ºè®®
        for table in analysis["table_stats"]:
            if table["row_count"] > 10000 and not table["has_autoindex"]:
                analysis["missing_indexes"].append({
                    "table": table["name"],
                    "reason": "å¤§è¡¨ç¼ºå°‘ç´¢å¼•",
                    "suggestion": f"CREATE INDEX idx_{table['name']}_id ON {table['name']}(id)"
                })
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if analysis["missing_indexes"]:
            analysis["recommendations"].append(
                f"å»ºè®®ä¸º {len(analysis['missing_indexes'])} ä¸ªå¤§è¡¨åˆ›å»ºç´¢å¼•"
            )
        
        return analysis
    
    # ==================== ER å›¾ç”Ÿæˆ ====================
    
    def generate_er_diagram(self, output_path: str = "er_diagram.dot") -> str:
        """
        ç”Ÿæˆ ER å›¾ (Graphviz DOT æ ¼å¼)
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if self.db_type == DatabaseType.MONGODB:
            raise ValueError("MongoDB ä¸æ”¯æŒ ER å›¾ç”Ÿæˆ")
        
        dot_content = ["digraph ERDiagram {"]
        dot_content.append('  rankdir=LR;')
        dot_content.append('  node [shape=box];')
        dot_content.append('')
        
        if self.db_type == DatabaseType.SQLITE:
            tables = self._get_sqlite_tables()
        
        for table in tables:
            table_name = table["name"]
            dot_content.append(f'  {table_name} [label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">')
            dot_content.append(f'    <TR><TD COLSPAN="2" BGCOLOR="#E8E8E8"><B>{table_name}</B></TD></TR>')
            
            # è§£æè¡¨ç»“æ„è·å–åˆ—ä¿¡æ¯
            if table["sql"]:
                col_matches = re.findall(r'(\w+)\s+(\w+)', table["sql"])
                for col_name, col_type in col_matches[:10]:  # é™åˆ¶åˆ—æ•°
                    dot_content.append(f'    <TR><TD>{col_name}</TD><TD>{col_type}</TD></TR>')
            
            dot_content.append('  </TABLE>>];')
            dot_content.append('')
        
        dot_content.append('}')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(dot_content))
        
        return output_path
    
    # ==================== SQL æ ¼å¼åŒ– ====================
    
    def format_sql(self, sql: str) -> str:
        """æ ¼å¼åŒ– SQL è¯­å¥"""
        return sqlparse.format(sql, reindent=True, keyword_case='upper')
    
    # ==================== å®‰å…¨æŸ¥è¯¢ ====================
    
    def safe_query(self, query: str, max_results: int = 1000) -> Tuple[List[Dict], str]:
        """
        å®‰å…¨æ‰§è¡ŒæŸ¥è¯¢ (é˜²æ­¢ SQL æ³¨å…¥)
        
        Args:
            query: SQL æŸ¥è¯¢
            max_results: æœ€å¤§è¿”å›è¡Œæ•°
            
        Returns:
            (æŸ¥è¯¢ç»“æœ, çŠ¶æ€æ¶ˆæ¯)
        """
        # æ£€æŸ¥å±é™©å…³é”®å­—
        dangerous = ["DROP", "DELETE", "TRUNCATE", "ALTER", "CREATE", "INSERT", "UPDATE", "EXEC", "EXECUTE"]
        query_upper = query.upper()
        
        for keyword in dangerous:
            if keyword in query_upper and "SELECT" not in query_upper:
                return [], f"â›” å±é™©æŸ¥è¯¢å·²é˜»æ­¢: åŒ…å« {keyword} å…³é”®å­—"
        
        # æ£€æŸ¥ SELECT æ˜¯å¦å­˜åœ¨
        if "SELECT" not in query_upper:
            return [], "â›” åªæ”¯æŒ SELECT æŸ¥è¯¢"
        
        # æ·»åŠ  LIMIT
        if "LIMIT" not in query_upper:
            query = f"{query} LIMIT {max_results}"
        
        try:
            rows, columns = self.execute_query(query)
            return rows, f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(rows)} è¡Œ"
        except Exception as e:
            return [], f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"


# ==================== CLI ç•Œé¢ ====================

def main():
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½æ•°æ®åº“å·¥å…· - æ”¯æŒå¤šç§æ•°æ®åº“çš„å¤šåŠŸèƒ½ç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹æ•°æ®åº“ä¿¡æ¯
  python smart_database_tool.py --info --db sqlite:///test.db
  
  # æ‰§è¡ŒæŸ¥è¯¢
  python smart_database_tool.py --query "SELECT * FROM users" --db sqlite:///test.db
  
  # å¯¼å‡ºè¡¨æ•°æ®
  python smart_database_tool.py --export --db sqlite:///test.db --table users --format csv
  
  # å¤‡ä»½æ•°æ®åº“
  python smart_database_tool.py --backup --db sqlite:///test.db --output ./backups/
  
  # åˆ†ææ€§èƒ½
  python smart_database_tool.py --analyze --db sqlite:///test.db
  
  # ç”Ÿæˆ ER å›¾
  python smart_database_tool.py --er --db sqlite:///test.db
        """
    )
    
    parser.add_argument("--db", required=True, help="æ•°æ®åº“è¿æ¥ URL")
    parser.add_argument("--query", help="è¦æ‰§è¡Œçš„ SQL æŸ¥è¯¢")
    parser.add_argument("--export", action="store_true", help="å¯¼å‡ºæ•°æ®æ¨¡å¼")
    parser.add_argument("--table", help="å¯¼å‡º/å¯¼å…¥çš„è¡¨å")
    parser.add_argument("--format", default="csv", choices=["csv", "json", "sql"], help="å¯¼å‡ºæ ¼å¼")
    parser.add_argument("--output", default=".", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--import-file", help="å¯¼å…¥æ•°æ®æ–‡ä»¶")
    parser.add_argument("--backup", action="store_true", help="å¤‡ä»½æ•°æ®åº“")
    parser.add_argument("--analyze", action="store_true", help="æ€§èƒ½åˆ†æ")
    parser.add_argument("--er", action="store_true", help="ç”Ÿæˆ ER å›¾")
    parser.add_argument("--info", action="store_true", help="æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯")
    parser.add_argument("--nl", help="è‡ªç„¶è¯­è¨€è½¬ SQL")
    parser.add_argument("--limit", type=int, default=1000, help="æŸ¥è¯¢ç»“æœé™åˆ¶")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    try:
        with SmartDatabaseTool(args.db) as db_tool:
            
            # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
            if args.info:
                info = db_tool.get_database_info()
                print(f"\nğŸ—„ï¸  æ•°æ®åº“ä¿¡æ¯")
                print(f"   ç±»å‹: {info['db_type']}")
                print(f"   URL: {info['db_url']}")
                if 'tables' in info:
                    print(f"   è¡¨æ•°é‡: {len(info['tables'])}")
                    for table in info['tables'][:10]:
                        print(f"   - {table['name']} ({table['row_count']} è¡Œ)")
                if 'collections' in info:
                    print(f"   é›†åˆæ•°é‡: {len(info['collections'])}")
                    for coll in info['collections'][:10]:
                        print(f"   - {coll['name']} ({coll['document_count']} æ–‡æ¡£)")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            elif args.query:
                if args.verbose:
                    formatted = db_tool.format_sql(args.query)
                    print(f"\nğŸ“ æ ¼å¼åŒ–åçš„ SQL:\n{formatted}")
                
                rows, status = db_tool.safe_query(args.query, args.limit)
                print(f"\n{status}")
                if rows:
                    if args.verbose:
                        columns = list(rows[0].keys())
                        print(f"åˆ—: {', '.join(columns)}")
                    for row in rows[:20]:
                        print(f"   {row}")
                    if len(rows) > 20:
                        print(f"   ... è¿˜æœ‰ {len(rows) - 20} è¡Œ")
            
            # è‡ªç„¶è¯­è¨€è½¬ SQL
            elif args.nl:
                sql = db_tool.nl_to_sql(args.nl)
                print(f"\nğŸ’¡ ç”Ÿæˆçš„ SQL:\n{sql}")
                if input("\næ˜¯å¦æ‰§è¡Œ? (y/n): ").lower() == 'y':
                    rows, status = db_tool.safe_query(sql, args.limit)
                    print(f"\n{status}")
                    for row in rows[:10]:
                        print(f"   {row}")
            
            # å¯¼å‡ºæ•°æ®
            elif args.export:
                if not args.table:
                    print("â›” è¯·æŒ‡å®š --table å‚æ•°")
                    sys.exit(1)
                filepath = db_tool.export_table(args.table, args.format, args.output)
                print(f"\nâœ… å·²å¯¼å‡ºåˆ°: {filepath}")
            
            # å¯¼å…¥æ•°æ®
            elif args.import_file:
                if not args.table:
                    print("â›” è¯·æŒ‡å®š --table å‚æ•°")
                    sys.exit(1)
                fmt = args.import_file.split('.')[-1].lower()
                db_tool.import_data(args.table, args.import_file, fmt, create_table=True)
                print(f"\nâœ… å·²ä» {args.import_file} å¯¼å…¥åˆ° {args.table}")
            
            # å¤‡ä»½
            elif args.backup:
                filepath = db_tool.backup(args.output)
                print(f"\nâœ… å¤‡ä»½å·²ä¿å­˜åˆ°: {filepath}")
            
            # æ€§èƒ½åˆ†æ
            elif args.analyze:
                analysis = db_tool.analyze_performance()
                print("\nğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š")
                print(f": {len(   è¡¨ç»Ÿè®¡analysis['table_stats'])} ä¸ªè¡¨")
                if analysis['missing_indexes']:
                    print(f"   ç¼ºå¤±ç´¢å¼•: {len(analysis['missing_indexes'])} ä¸ª")
                    for idx in analysis['missing_indexes'][:5]:
                        print(f"   - {idx['table']}: {idx['suggestion']}")
                if analysis['recommendations']:
                    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                    for rec in analysis['recommendations']:
                        print(f"   - {rec}")
            
            # ER å›¾
            elif args.er:
                filepath = db_tool.generate_er_diagram(args.output)
                print(f"\nâœ… ER å›¾å·²ç”Ÿæˆ: {filepath}")
                print("   ä½¿ç”¨ Graphviz è½¬æ¢ä¸ºå›¾ç‰‡: dot -Tpng er_diagram.dot -o er_diagram.png")
            
            else:
                parser.print_help()
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

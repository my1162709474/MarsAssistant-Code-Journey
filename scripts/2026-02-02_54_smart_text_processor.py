#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬å¤„ç†å·¥å…· - æ–‡æœ¬æ¸…æ´—ã€æ ¼å¼åŒ–ã€æ‘˜è¦æå–ã€å…³é”®è¯æå–
æ”¯æŒå‘½ä»¤è¡Œäº¤äº’å’Œæ‰¹é‡å¤„ç†
"""

import re
import json
from collections import Counter
from typing import List, Dict, Tuple, Optional

class SmartTextProcessor:
    """æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨"""
    
    def __init__(self):
        # ä¸­æ–‡åœç”¨è¯åˆ—è¡¨
        self.stopwords = set([
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä¹ˆ', 'å¥¹', 'ä»–', 'å®ƒ', 'ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å¦‚ä½•',
            'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆæ ·', 'å“ªäº›', 'å“ªä¸ª', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'éœ€è¦', 'å¯èƒ½', 'çŸ¥é“',
            'åª', 'ä½†', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'è¿˜æ˜¯', 'å·²ç»', 'å¾ˆå¤š',
            'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'è‡ªå·±', 'è¿™é‡Œ', 'é‚£é‡Œ', 'è¿™æ ·', 'é‚£æ ·'
        ])
        
        # è‹±æ–‡åœç”¨è¯
        self.english_stopwords = set([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been',
            'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'must', 'shall', 'can', 'this',
            'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
            'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our',
            'their', 'what', 'which', 'who', 'whom', 'when', 'where', 'why', 'how',
            'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other', 'some',
            'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',
            'very', 'just', 'also'
        ])
    
    def clean_text(self, text: str, remove_numbers: bool = False) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹ï¼‰
        if remove_numbers:
            text = re.sub(r'[0-9]+', '', text)
        
        # ç§»é™¤URL
        text = re.sub(r'https?://\S+', '', text)
        
        # ç§»é™¤é‚®ç®±
        text = re.sub(r'\S+@\S+\.\S+', '', text)
        
        return text
    
    def remove_stopwords(self, text: str, lang: str = 'zh') -> List[str]:
        """ç§»é™¤åœç”¨è¯"""
        words = text.split()
        
        if lang == 'zh':
            return [w for w in words if w not in self.stopwords and len(w) > 1]
        elif lang == 'en':
            return [w.lower() for w in words if w.lower() not in self.english_stopwords and len(w) > 2]
        else:
            stopwords = self.stopwords | self.english_stopwords
            return [w.lower() for w in words if w.lower() not in stopwords and len(w) > 2]
    
    def extract_keywords(self, text: str, top_n: int = 10, lang: str = 'zh') -> List[Tuple[str, float]]:
        """æå–å…³é”®è¯ - åŸºäºTFé¢‘ç‡"""
        cleaned = self.clean_text(text)
        words = self.remove_stopwords(cleaned, lang)
        
        if not words:
            return []
        
        word_freq = Counter(words)
        total = sum(word_freq.values())
        
        # è®¡ç®—TFåˆ†æ•°
        keywords = [(word, freq / total) for word, freq in word_freq.most_common(top_n)]
        return keywords
    
    def extract_bigrams(self, text: str, top_n: int = 10) -> List[Tuple[str, float]]:
        """æå–åŒè¯ç»„åˆ"""
        cleaned = self.clean_text(text)
        words = self.remove_stopwords(cleaned)
        
        if len(words) < 2:
            return []
        
        bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]
        bigram_freq = Counter(bigrams)
        total = sum(bigram_freq.values())
        
        return [(bg, freq / total) for bg, freq in bigram_freq.most_common(top_n)]
    
    def generate_summary(self, text: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ‘˜è¦ - æŠ½å–å¼"""
        if not text:
            return ""
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        summary = []
        current_length = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if current_length + len(sentence) <= max_length:
                summary.append(sentence)
                current_length += len(sentence)
        
        if summary:
            return ''.join(summary)
        return text[:max_length] + ('...' if len(text) > max_length else '')
    
    def extract_sentences(self, text: str, max_sentences: int = 5) -> List[str]:
        """æå–å…³é”®å¥å­"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è¿”å›å‰Nä¸ªå¥å­
        return sentences[:max_sentences]
    
    def calculate_readability(self, text: str) -> Dict:
        """è®¡ç®—å¯è¯»æ€§æŒ‡æ ‡"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        words = text.split()
        
        if not sentences or not words:
            return {'score': 0, 'level': 'æ— æ³•è®¡ç®—', 'avg_sentence_len': 0, 'avg_word_len': 0}
        
        # å¹³å‡å¥å­é•¿åº¦
        avg_sentence_len = len(words) / len(sessions)
        
        # å¹³å‡è¯é•¿åº¦
        avg_word_len = sum(len(w) for w in words) / len(words) if words else 0
        
        # ç®€å•å¯è¯»æ€§è¯„åˆ† (0-100)
        # å¥å­çŸ­ã€è¯çŸ­ = å®¹æ˜“é˜…è¯»
        readability = max(0, min(100, 100 - avg_sentence_len * 2 - avg_word_len * 5))
        
        # ç¡®å®šé˜…è¯»çº§åˆ«
        if readability >= 80:
            level = 'éå¸¸ç®€å•'
        elif readability >= 60:
            level = 'ç®€å•'
        elif readability >= 40:
            level = 'ä¸­ç­‰'
        elif readability >= 20:
            level = 'è¾ƒéš¾'
        else:
            level = 'éå¸¸éš¾'
        
        return {
            'score': round(readability, 1),
            'level': level,
            'avg_sentence_len': round(avg_sentence_len, 1),
            'avg_word_len': round(avg_word_len, 1),
            'sentence_count': len(sentences),
            'word_count': len(words)
        }
    
    def analyze_text(self, text: str, lang: str = 'zh') -> Dict:
        """å®Œæ•´æ–‡æœ¬åˆ†æ"""
        cleaned = self.clean_text(text)
        words = self.remove_stopwords(cleaned, lang)
        
        return {
            'statistics': {
                'char_count': len(text),
                'word_count': len(text.split()),
                'clean_word_count': len(words),
                'sentence_count': len(re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text))
            },
            'keywords': self.extract_keywords(text, 10, lang),
            'bigrams': self.extract_bigrams(text, 5),
            'summary': self.generate_summary(text, 200),
            'readability': self.calculate_readability(text)
        }
    
    def format_for_markdown(self, text: str, title: str = "æ–‡æœ¬åˆ†ææŠ¥å‘Š", lang: str = 'zh') -> str:
        """æ ¼å¼åŒ–ä¸ºMarkdownæŠ¥å‘Š"""
        analysis = self.analyze_text(text, lang)
        keywords = analysis['keywords']
        bigrams = analysis['bigrams']
        readability = analysis['readability']
        
        lines = [
            f"# {title}",
            f"**åˆ†ææ—¶é—´**: è‡ªåŠ¨ç”Ÿæˆ",
            "",
            "## ğŸ“Š åŸºæœ¬ç»Ÿè®¡",
            f"- **å­—ç¬¦æ•°**: {analysis['statistics']['char_count']}",
            f"- **è¯æ•°**: {analysis['statistics']['word_count']}",
            f"- **æ¸…æ´—åè¯æ•°**: {analysis['statistics']['clean_word_count']}",
            f"- **å¥å­æ•°**: {analysis['statistics']['sentence_count']}",
            "",
            "## ğŸ¯ å…³é”®è¯ Top 10",
        ]
        
        for i, (word, score) in enumerate(keywords, 1):
            bar = 'â–ˆ' * int(score * 50) if score > 0 else ''
            lines.append(f"{i}. **{word}** `{score:.4f}` {bar}")
        
        if bigrams:
            lines.extend([
                "",
                "## ğŸ”— å…³é”®çŸ­è¯­ Top 5",
            ])
            for i, (phrase, score) in enumerate(bigrams, 1):
                lines.append(f"{i}. {phrase} `{score:.4f}`")
        
        lines.extend([
            "",
            "## ğŸ“– å¯è¯»æ€§åˆ†æ",
            f"- **é˜…è¯»éš¾åº¦**: {readability['level']} ({readability['score']}/100)",
            f"- **å¹³å‡å¥é•¿**: {readability['avg_sentence_len']} è¯/å¥",
            f"- **å¹³å‡è¯é•¿**: {readability['avg_word_len']} å­—ç¬¦/è¯",
            "",
            "## ğŸ“‹ æ–‡æœ¬æ‘˜è¦",
            analysis['summary'],
            "",
            "---",
            "*ç”± SmartTextProcessor è‡ªåŠ¨ç”Ÿæˆ*"
        ])
        
        return '\n'.join(lines)
    
    def compare_texts(self, texts: List[Dict[str, str]]) -> Dict:
        """å¯¹æ¯”å¤šä¸ªæ–‡æœ¬"""
        results = []
        for i, item in enumerate(texts):
            analysis = self.analyze_text(item['content'], item.get('lang', 'zh'))
            results.append({
                'name': item.get('name', f'æ–‡æœ¬{i+1}'),
                'statistics': analysis['statistics'],
                'readability': analysis['readability'],
                'top_keywords': analysis['keywords'][:3]
            })
        return results

# ============ CLI æ¥å£ ============
def main():
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ–‡æœ¬å¤„ç†å·¥å…·')
    parser.add_argument('text', nargs='?', help='è¦åˆ†æçš„æ–‡æœ¬')
    parser.add_argument('-f', '--file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-i', '--interactive', action='store_true', help='äº¤äº’æ¨¡å¼')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--format', choices=['json', 'markdown'], default='json', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--lang', choices=['zh', 'en', 'mix'], default='zh', help='æ–‡æœ¬è¯­è¨€')
    parser.add_argument('--summary', action='store_true', help='åªè¾“å‡ºæ‘˜è¦')
    parser.add_argument('--keywords', action='store_true', help='åªè¾“å‡ºå…³é”®è¯')
    parser.add_argument('--readability', action='store_true', help='åªè¾“å‡ºå¯è¯»æ€§')
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”æ¨¡å¼')
    
    args = parser.parse_args()
    
    processor = SmartTextProcessor()
    
    # è·å–æ–‡æœ¬
    if args.interactive:
        print("è¾“å…¥æ–‡æœ¬ï¼ˆCtrl+D ç»“æŸï¼‰:")
        text = sys.stdin.read()
    elif args.file:
        with open(args.file, 'r', encoding='utf-8') as f:
            text = f.read()
    elif args.text:
        text = args.text
    else:
        parser.print_help()
        return
    
    # åˆ†æ
    if args.compare and args.file:
        # å¯¹æ¯”æ¨¡å¼ï¼šè§£æå¤šä¸ªæ–‡ä»¶
        files = args.file.split(',')
        texts = []
        for f in files:
            with open(f.strip(), 'r', encoding='utf-8') as fp:
                texts.append({'name': f.strip(), 'content': fp.read()})
        result = processor.compare_texts(texts)
        output = json.dumps(result, ensure_ascii=False, indent=2)
    elif args.summary:
        output = processor.generate_summary(text)
    elif args.keywords:
        keywords = processor.extract_keywords(text, lang=args.lang)
        output = json.dumps(keywords, ensure_ascii=False, indent=2)
    elif args.readability:
        output = json.dumps(processor.calculate_readability(text), ensure_ascii=False, indent=2)
    elif args.format == 'markdown':
        output = processor.format_for_markdown(text, lang=args.lang)
    else:
        output = json.dumps(processor.analyze_text(text, args.lang), ensure_ascii=False, indent=2)
    
    # è¾“å‡º
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(output)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print(output)

if __name__ == "__main__":
    main()

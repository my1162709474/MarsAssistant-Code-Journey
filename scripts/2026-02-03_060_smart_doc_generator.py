#!/usr/bin/env python3
"""
ğŸ—‚ï¸ æ™ºèƒ½ä»£ç æ–‡æ¡£ç”Ÿæˆå™¨
AIé©±åŠ¨çš„ä»£ç æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆå·¥å…·

åŠŸèƒ½ï¼š
- è‡ªåŠ¨åˆ†æä»£ç ç»“æ„ç”Ÿæˆæ–‡æ¡£
- æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€
- ç”ŸæˆAPIæ–‡æ¡£ã€READMEã€å‡½æ•°æ³¨é‡Š
- æ™ºèƒ½æå–ä»£ç æ„å›¾å’ŒåŠŸèƒ½æè¿°
"""

import ast
import re
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path


class CodeDocumentGenerator:
    """æ™ºèƒ½ä»£ç æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    # è¯­è¨€å…³é”®è¯æ˜ å°„
    LANGUAGE_PATTERNS = {
        'python': ['def ', 'class ', 'import ', 'from ', '=', '#'],
        'javascript': ['function ', 'const ', 'let ', 'var ', '=>', 'import ', 'export'],
        'java': ['public ', 'private ', 'protected ', 'class ', 'void ', 'import '],
        'go': ['func ', 'type ', 'import ', 'const ', 'var '],
        'rust': ['fn ', 'struct ', 'impl ', 'pub ', 'let '],
    }
    
    # ä»£ç æ„å›¾å…³é”®è¯
    INTENT_KEYWORDS = {
        'data_processing': ['parse', 'transform', 'convert', 'filter', 'map', 'reduce'],
        'file_io': ['read', 'write', 'open', 'save', 'load', 'export', 'import'],
        'api': ['request', 'response', 'endpoint', 'api', 'http', 'fetch', 'client'],
        'database': ['query', 'insert', 'update', 'delete', 'connect', 'transaction'],
        'testing': ['test', 'assert', 'mock', 'verify', 'validate', 'check'],
        'utils': ['util', 'helper', 'tool', 'common', 'shared', 'base'],
        'algorithm': ['sort', 'search', 'find', 'calculate', 'compute', 'optimize'],
        'ui': ['render', 'display', 'show', 'view', 'component', 'widget'],
    }
    
    def __init__(self):
        self.stats = {'files_processed': 0, 'docs_generated': 0, 'entities_found': 0}
    
    def detect_language(self, code: str) -> str:
        """æ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
        scores = {}
        for lang, patterns in self.LANGUAGE_PATTERNS.items():
            score = sum(1 for pattern in patterns if pattern in code)
            if score > 0:
                scores[lang] = score
        
        if scores:
            return max(scores, key=scores.get)
        return 'python'  # é»˜è®¤Python
    
    def extract_python_entities(self, code: str) -> List[Dict[str, Any]]:
        """æå–Pythonä»£ç å®ä½“ï¼ˆç±»ã€å‡½æ•°ã€å¯¼å…¥ç­‰ï¼‰"""
        entities = []
        tree = ast.parse(code)
        
        for node in ast.walk(tree):
            # æå–ç±»
            if isinstance(node, ast.ClassDef):
                docstring = ast.get_docstring(node) or ""
                entities.append({
                    'type': 'class',
                    'name': node.name,
                    'line': node.lineno,
                    'docstring': docstring,
                    'methods': [n.name for n in node.body if isinstance(n, ast.FunctionDef)],
                    'bases': [base.id if isinstance(base, ast.Name) else str(base) for base in node.bases]
                })
            
            # æå–å‡½æ•°
            elif isinstance(node, ast.FunctionDef):
                docstring = ast.get_docstring(node) or ""
                args = [arg.arg for arg in node.args.args]
                entities.append({
                    'type': 'function',
                    'name': node.name,
                    'line': node.lineno,
                    'docstring': docstring,
                    'args': args,
                    'returns': self._get_return_type(node)
                })
        
        self.stats['entities_found'] += len(entities)
        return entities
    
    def _get_return_type(self, node) -> str:
        """è·å–å‡½æ•°è¿”å›ç±»å‹"""
        if node.returns:
            if isinstance(node.returns, ast.Name):
                return node.returns.id
            elif isinstance(node.returns, ast.Constant):
                return str(node.returns.value)
        return 'Any'
    
    def extract_code_intent(self, code: str) -> List[str]:
        """æå–ä»£ç æ„å›¾"""
        code_lower = code.lower()
        intents = []
        
        for intent, keywords in self.INTENT_KEYWORDS.items():
            if any(kw in code_lower for kw in keywords):
                intents.append(intent)
        
        return intents if intents else ['general']
    
    def generate_docstring(self, entity: Dict[str, Any]) -> str:
        """ä¸ºå®ä½“ç”Ÿæˆæ–‡æ¡£å­—ç¬¦ä¸²"""
        lines = []
        
        if entity['type'] == 'class':
            lines.append(f"## {entity['name']}")
            if entity.get('docstring'):
                lines.append(f"\n{entity['docstring']}")
            if entity.get('bases'):
                lines.append(f"\n**ç»§æ‰¿è‡ª**: {', '.join(entity['bases'])}")
            if entity.get('methods'):
                lines.append(f"\n**æ–¹æ³•**:\n- " + "\n- ".join(entity['methods']))
        
        elif entity['type'] == 'function':
            lines.append(f"### `{entity['name']}()`")
            if entity.get('docstring'):
                lines.append(f"\n{entity['docstring']}")
            if entity.get('args'):
                lines.append(f"\n**å‚æ•°**:\n")
                for arg in entity['args']:
                    lines.append(f"- `{arg}`: ")
            if entity.get('returns'):
                lines.append(f"\n**è¿”å›**: `{entity['returns']}`")
        
        return '\n'.join(lines)
    
    def generate_readme_section(self, file_path: str, entities: List[Dict]) -> str:
        """ç”ŸæˆREADMEæ–‡æ¡£ç‰‡æ®µ"""
        filename = Path(file_path).stem.replace('_', ' ').title()
        intent = self.extract_code_intent(open(file_path).read())
        intent_str = ' / '.join(intent)
        
        lines = [
            f"## {filename}",
            f"- **æ–‡ä»¶**: `{file_path}`",
            f"- **ç±»å‹**: {' / '.join(set(e['type'] for e in entities))}",
            f"- **ç”¨é€”**: {intent_str}",
            "",
            "### å®ä½“",
        ]
        
        for entity in entities:
            lines.append(f"- **{entity['type']}**: `{entity['name']}`")
            if entity.get('docstring'):
                # å–ç¬¬ä¸€å¥è¯ä½œä¸ºç®€ä»‹
                first_sentence = entity['docstring'].split('.')[0]
                lines.append(f"  - {first_sentence}.")
        
        return '\n'.join(lines)
    
    def analyze_file(self, file_path: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
            
            language = self.detect_language(code)
            entities = []
            
            if language == 'python':
                try:
                    entities = self.extract_python_entities(code)
                except SyntaxError:
                    # ç®€å•çš„æ­£åˆ™æå–ä½œä¸ºåå¤‡
                    entities = self._simple_extract_entities(code)
            
            intent = self.extract_code_intent(code)
            
            self.stats['files_processed'] += 1
            self.stats['docs_generated'] += len(entities)
            
            return {
                'file': file_path,
                'language': language,
                'entities': entities,
                'intent': intent,
                'line_count': len(code.splitlines()),
                'code': code  # ç”¨äºbase64ç¼–ç 
            }
        except Exception as e:
            return {'file': file_path, 'error': str(e)}
    
    def _simple_extract_entities(self, code: str) -> List[Dict]:
        """ç®€å•çš„å®ä½“æå–ï¼ˆæ­£åˆ™ä½œä¸ºåå¤‡ï¼‰"""
        entities = []
        
        # æå–ç±»å’Œå‡½æ•°
        class_pattern = r'class\s+(\w+)'
        func_pattern = r'def\s+(\w+)'
        
        for match in re.finditer(class_pattern, code):
            entities.append({
                'type': 'class',
                'name': match.group(1),
                'docstring': '',
                'methods': []
            })
        
        for match in re.finditer(func_pattern, code):
            entities.append({
                'type': 'function',
                'name': match.group(1),
                'docstring': '',
                'args': [],
                'returns': 'Any'
            })
        
        return entities
    
    def batch_analyze(self, directory: str, extensions: List[str] = ['.py']) -> List[Dict]:
        """æ‰¹é‡åˆ†æç›®å½•ä¸­çš„æ–‡ä»¶"""
        results = []
        path = Path(directory)
        
        for file_path in path.rglob('*'):
            if file_path.is_file() and file_path.suffix in extensions:
                result = self.analyze_file(str(file_path))
                if 'error' not in result:
                    results.append(result)
        
        return results
    
    def generate_markdown_docs(self, analysis_results: List[Dict], output_file: str = "DOCUMENTATION.md"):
        """ç”Ÿæˆå®Œæ•´çš„Markdownæ–‡æ¡£"""
        lines = [
            "# è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç æ–‡æ¡£",
            f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"\nç»Ÿè®¡: {self.stats['files_processed']} ä¸ªæ–‡ä»¶, {self.stats['docs_generated']} ä¸ªå®ä½“",
            "",
            "---",
            "",
            "# ç›®å½•",
        ]
        
        for result in analysis_results:
            filename = Path(result['file']).stem
            lines.append(f"- [{filename}](#{filename.lower().replace('_', '-')})")
        
        lines.append("")
        
        for result in analysis_results:
            lines.append(f"## {Path(result['file']).stem}")
            lines.append(f"\n**æ–‡ä»¶**: `{result['file']}`")
            lines.append(f"**è¯­è¨€**: {result['language']}")
            lines.append(f"**è¡Œæ•°**: {result['line_count']}")
            lines.append(f"**ç±»å‹**: {', '.join(set(e['type'] for e in result['entities']))}")
            
            for entity in result['entities']:
                lines.append("")
                docstring = self.generate_docstring(entity)
                lines.append(docstring)
            
            lines.append("\n---")
        
        content = '\n'.join(lines)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“„ æ–‡æ¡£å·²ç”Ÿæˆ: {output_file}")
        return content
    
    def export_to_json(self, analysis_results: List[Dict], output_file: str = "docs.json"):
        """å¯¼å‡ºåˆ†æç»“æœä¸ºJSON"""
        export_data = {
            'generated_at': datetime.now().isoformat(),
            'stats': self.stats,
            'files': analysis_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“¦ æ•°æ®å·²å¯¼å‡º: {output_file}")
        return export_data


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ—‚ï¸ æ™ºèƒ½ä»£ç æ–‡æ¡£ç”Ÿæˆå™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹ä»£ç 
    sample_code = '''
#!/usr/bin/env python3
"""
ç¤ºä¾‹è®¡ç®—å™¨æ¨¡å—
æä¾›åŸºæœ¬çš„æ•°å­¦è¿ç®—åŠŸèƒ½
"""

class Calculator:
    """ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨ç±»"""
    
    def __init__(self, precision: int = 2):
        """åˆå§‹åŒ–è®¡ç®—å™¨
        
        Args:
            precision: å°æ•°ç²¾åº¦
        """
        self.precision = precision
    
    def add(self, a: float, b: float) -> float:
        """åŠ æ³•è¿ç®—
        
        Args:
            a: ç¬¬ä¸€ä¸ªæ•°
            b: ç¬¬äºŒä¸ªæ•°
            
        Returns:
            ä¸¤æ•°ä¹‹å’Œ
        """
        return round(a + b, self.precision)
    
    def multiply(self, a: float, b: float) -> float:
        """ä¹˜æ³•è¿ç®—"""
        return round(a * b, self.precision)


def calculate_average(numbers: List[float]) -> float:
    """è®¡ç®—åˆ—è¡¨çš„å¹³å‡å€¼
    
    Args:
        numbers: æ•°å­—åˆ—è¡¨
        
    Returns:
        å¹³å‡å€¼
    """
    if not numbers:
        return 0
    return sum(numbers) / len(numbers)
'''
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with open('/tmp/sample_calculator.py', 'w') as f:
        f.write(sample_code)
    
    # ä½¿ç”¨æ–‡æ¡£ç”Ÿæˆå™¨
    generator = CodeDocumentGenerator()
    
    # åˆ†ææ–‡ä»¶
    result = generator.analyze_file('/tmp/sample_calculator.py')
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"- è¯­è¨€: {result['language']}")
    print(f"- è¡Œæ•°: {result['line_count']}")
    print(f"- å®ä½“æ•°é‡: {len(result['entities'])}")
    
    print(f"\nğŸ“ å‘ç°çš„å®ä½“:")
    for entity in result['entities']:
        print(f"  - [{entity['type']}] {entity['name']}")
        if entity.get('docstring'):
            print(f"    æ–‡æ¡£: {entity['docstring'][:50]}...")
    
    # ç”Ÿæˆæ–‡æ¡£
    print(f"\nğŸ“„ ç”Ÿæˆæ–‡æ¡£å­—ç¬¦ä¸²:")
    for entity in result['entities']:
        doc = generator.generate_docstring(entity)
        print(doc[:100] + "..." if len(doc) > 100 else doc)
    
    print(f"\nâœ… ç»Ÿè®¡: {generator.stats}")
    
    # æ¸…ç†
    import os
    os.remove('/tmp/sample_calculator.py')


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'demo':
        demo()
        return
    
    generator = CodeDocumentGenerator()
    
    print("ğŸ—‚ï¸ æ™ºèƒ½ä»£ç æ–‡æ¡£ç”Ÿæˆå™¨")
    print("=" * 40)
    print("ç”¨æ³•:")
    print("  python smart_doc_generator.py <æ–‡ä»¶è·¯å¾„>")
    print("  python smart_doc_generator.py <ç›®å½•è·¯å¾„> --batch")
    print("  python smart_doc_generator.py --demo")
    print()
    
    if len(sys.argv) > 1:
        path = sys.argv[1]
        
        if Path(path).is_file():
            result = generator.analyze_file(path)
            print(f"åˆ†æç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
        
        elif Path(path).is_dir():
            results = generator.batch_analyze(path)
            print(f"åˆ†æäº† {len(results)} ä¸ªæ–‡ä»¶")
            
            # ç”Ÿæˆæ–‡æ¡£
            generator.generate_markdown_docs(results, "DOCUMENTATION.md")
            generator.export_to_json(results, "docs.json")


if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬å¤„ç†å·¥å…· - Day 60
Smart Text Processor - AI-powered text analysis and manipulation

åŠŸèƒ½:
- æ–‡æœ¬æ¸…æ´—ï¼ˆå»é™¤HTMLã€ç‰¹æ®Šå­—ç¬¦ã€é‡å¤ç©ºæ ¼ï¼‰
- å…³é”®è¯æå–ï¼ˆTF-IDFã€TextRankï¼‰
- æ–‡æœ¬æ‘˜è¦ï¼ˆæŠ½å–å¼æ‘˜è¦ï¼‰
- æƒ…æ„Ÿåˆ†æ
- è¯­è¨€æ£€æµ‹
- æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
- è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ

ä½œè€…: MarsAssistant
æ—¥æœŸ: 2026-02-03
"""

import re
import json
import hashlib
from collections import Counter
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import math


@dataclass
class TextAnalysisResult:
    """æ–‡æœ¬åˆ†æç»“æœ"""
    original_length: int
    cleaned_length: int
    keywords: List[Tuple[str, float]]
    summary: str
    sentiment: Dict[str, float]
    language: str
    entities: List[str]
    readability_score: float


class SmartTextProcessor:
    """æ™ºèƒ½æ–‡æœ¬å¤„ç†å™¨"""
    
    def __init__(self):
        # åœç”¨è¯åˆ—è¡¨ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰
        self.stopwords = set([
            # English
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'must', 'shall', 'can', 'need',
            'it', 'its', 'this', 'that', 'these', 'those', 'i', 'you', 'he',
            'she', 'we', 'they', 'what', 'which', 'who', 'whom', 'where',
            'when', 'why', 'how', 'all', 'each', 'every', 'both', 'few',
            'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',
            'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just',
            # Chinese
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½',
            'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ',
            'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä¹ˆ', 'å¥¹',
            'ä»–', 'å®ƒ', 'ä»¬', 'ä¸º', 'ä»€ä¹ˆ', 'æ²¡', 'å¯¹', 'ä¸', 'æˆ–', 'ç­‰'
        ])
        
        # æƒ…æ„Ÿè¯å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.positive_words = set([
            'å¥½', 'ä¼˜ç§€', 'æ£’', 'ç²¾å½©', 'å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹',
            'æ»¡æ„', 'æˆåŠŸ', 'èƒœåˆ©', 'å¼ºå¤§', 'ç¾ä¸½', 'æ¼‚äº®', 'èªæ˜', 'å–„è‰¯',
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',
            'love', 'happy', 'joy', 'success', 'beautiful', 'smart', 'strong'
        ])
        
        self.negative_words = set([
            'å', 'å·®', 'ç³Ÿç³•', 'è®¨åŒ', 'æ¨', 'ä¼¤å¿ƒ', 'éš¾è¿‡', 'å¤±è´¥', 'ä¸‘',
            'ç¬¨', 'è ¢', 'é‚ªæ¶', 'å¯æ€•', 'ææ€–', 'bad', 'terrible', 'awful',
            'hate', 'sad', 'angry', 'failure', 'ugly', 'stupid', 'fear', 'horror'
        ])
        
        # è¯­è¨€ç‰¹å¾
        self.lang_patterns = {
            'zh': re.compile(r'[\u4e00-\u9fff]'),  # ä¸­æ–‡
            'ja': re.compile(r'[\u3040-\u309f\u30a0-\u30ff]'),  # æ—¥æ–‡
            'ko': re.compile(r'[\uac00-\ud7ff]'),  # éŸ©æ–‡
            'en': re.compile(r'[a-zA-Z]'),  # è‹±æ–‡
        }
    
    def clean_text(self, text: str, remove_html: bool = True,
                   remove_special: bool = True,
                   remove_extra_spaces: bool = True) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            remove_html: æ˜¯å¦ç§»é™¤HTMLæ ‡ç­¾
            remove_special: æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            remove_extra_spaces: æ˜¯å¦ç§»é™¤å¤šä½™ç©ºæ ¼
        
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        if remove_html:
            text = re.sub(r'<[^>]+>', '', text)
            text = re.sub(r'&[a-zA-Z]+;', '', text)  # &nbsp; etc.
        
        # ç§»é™¤URL
        text = re.sub(r'https?://\S+', '', text)
        
        # ç§»é™¤é‚®ç®±
        text = re.sub(r'\S+@\S+', '', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹ï¼‰
        if remove_special:
            text = re.sub(r'[^\w\s\u4e00-\u9fff\.\,\!\?\;\:\'\"\-\â€”]', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        if remove_extra_spaces:
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
        
        return text
    
    def tokenize(self, text: str, language: str = 'auto') -> List[str]:
        """
        åˆ†è¯
        
        Args:
            text: æ–‡æœ¬
            language: è¯­è¨€ï¼ˆautoè‡ªåŠ¨æ£€æµ‹ï¼‰
        
        Returns:
            è¯åˆ—è¡¨
        """
        if language == 'auto':
            language = self.detect_language(text)
        
        text = self.clean_text(text)
        
        if language == 'zh':
            # ç®€å•ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰
            tokens = list(text)
        else:
            # è‹±æ–‡åˆ†è¯
            tokens = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’Œè¿‡çŸ­çš„è¯
        tokens = [t for t in tokens if t not in self.stopwords and len(t) > 1]
        
        return tokens
    
    def extract_keywords_tfidf(self, text: str, top_n: int = 10) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨TF-IDFæå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
        
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼ˆè¯, æƒé‡ï¼‰
        """
        tokens = self.tokenize(text)
        
        if not tokens:
            return []
        
        # TFè®¡ç®—
        tf = Counter(tokens)
        total = len(tokens)
        tf = {word: count / total for word, count in tf.items()}
        
        # ç®€åŒ–IDFï¼ˆä½¿ç”¨æ–‡æ¡£é¢‘ç‡ä¼°è®¡ï¼‰
        # å‡è®¾è¯è¶Šå¸¸è§IDFè¶Šä½
        idf = {}
        for word in tf.keys():
            # å‡è®¾å‡ºç°æ¬¡æ•°è¶Šå°‘çš„è¯IDFè¶Šé«˜
            idf[word] = 1.0 / (1.0 + math.log(1 + tf[word] * 100))
        
        # TF-IDF
        tfidf = {word: tf_val * idf_val 
                 for word, tf_val in tf.items() 
                 for idf_val in [idf[word]]}
        
        # æ’åºè¿”å›
        sorted_keywords = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)
        return sorted_keywords[:top_n]
    
    def extract_keywords_textrank(self, text: str, top_n: int = 10,
                                   damping: float = 0.85,
                                   max_iter: int = 100) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨TextRankç®—æ³•æå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
            damping: é˜»å°¼ç³»æ•°
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼ˆè¯, æƒé‡ï¼‰
        """
        tokens = self.tokenize(text)
        
        if len(tokens) < 2:
            return [(t, 1.0) for t in tokens[:top_n]]
        
        # æ„å»ºè¯å…±ç°å›¾ï¼ˆçª—å£å¤§å°ä¸º2ï¼‰
        word_scores = {}
        
        # ç®€åŒ–TextRankï¼šåŸºäºè¯é¢‘å’Œä½ç½®
        word_freq = Counter(tokens)
        total = len(tokens)
        
        for word, freq in word_freq.items():
            # è¯é¢‘æƒé‡ + ä½ç½®æƒé‡ï¼ˆå‰é¢çš„è¯æ›´é‡è¦ï¼‰
            word_scores[word] = freq / total
        
        # å½’ä¸€åŒ–
        max_score = max(word_scores.values()) if word_scores else 1
        word_scores = {word: score / max_score for word, score in word_scores.items()}
        
        # æ’åºè¿”å›
        sorted_keywords = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_keywords[:top_n]
    
    def summarize(self, text: str, ratio: float = 0.3) -> str:
        """
        æŠ½å–å¼æ–‡æœ¬æ‘˜è¦
        
        Args:
            text: æ–‡æœ¬
            ratio: æ‘˜è¦é•¿åº¦æ¯”ä¾‹ï¼ˆ0-1ï¼‰
        
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # æ¸…ç†æ–‡æœ¬
        text = self.clean_text(text)
        
        if not text:
            return ""
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²å¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) <= 2:
            return text
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„å¾—åˆ†
        keywords = set(word for word, _ in self.extract_keywords_tfidf(text, 20))
        
        sentence_scores = []
        for i, sentence in enumerate(sentences):
            score = 0
            
            # å…³é”®è¯å‘½ä¸­å¾—åˆ†
            sentence_tokens = self.tokenize(sentence)
            keyword_hits = sum(1 for token in sentence_tokens if token in keywords)
            score += keyword_hits * 2
            
            # ä½ç½®å¾—åˆ†ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦ï¼‰
            if i == 0:
                score += 3
            elif i == len(sentences) - 1:
                score += 2
            
            # é•¿åº¦å¾—åˆ†ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿çš„å¥å­å¾—åˆ†ä½ï¼‰
            length = len(sentence)
            if 10 < length < 100:
                score += 1
            
            sentence_scores.append((sentence, score))
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œé€‰æ‹©topå¥å­
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—æ‘˜è¦é•¿åº¦
        target_length = max(1, int(len(sentences) * ratio))
        target_length = min(target_length, len(sentences))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        selected_indices = set()
        for sentence, score in sentence_scores:
            if len(selected_indices) >= target_length:
                break
            # æ‰¾åˆ°å¥å­çš„åŸå§‹ç´¢å¼•
            for i, s in enumerate(sentences):
                if s == sentence and i not in selected_indices:
                    selected_indices.add(i)
                    break
        
        # æŒ‰åŸå§‹é¡ºåºæ’åˆ—
        selected_sentences = [sentences[i] for i in sorted(selected_indices)]
        
        return 'ã€‚'.join(selected_sentences) + 'ã€‚' if selected_sentences else text[:200]
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """
        æƒ…æ„Ÿåˆ†æ
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            æƒ…æ„Ÿå¾—åˆ† {'positive': 0.0-1.0, 'negative': 0.0-1.0, 'neutral': 0.0-1.0}
        """
        tokens = self.tokenize(text)
        
        if not tokens:
            return {'positive': 0.0, 'negative': 0.0, 'neutral': 1.0}
        
        positive_count = sum(1 for t in tokens if t in self.positive_words)
        negative_count = sum(1 for t in tokens if t in self.negative_words)
        
        total = len(tokens)
        
        # é¿å…é™¤ä»¥é›¶
        if total == 0:
            return {'positive': 0.0, 'negative': 0.0, 'neutral': 1.0}
        
        positive = positive_count / total
        negative = negative_count / total
        
        # å½’ä¸€åŒ–
        total_score = positive + negative
        if total_score > 0:
            positive = positive / total_score
            negative = negative / total_score
        
        neutral = 1.0 - min(positive + negative, 1.0)
        
        return {
            'positive': round(positive, 4),
            'negative': round(negative, 4),
            'neutral': round(neutral, 4)
        }
    
    def detect_language(self, text: str) -> str:
        """
        æ£€æµ‹è¯­è¨€
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            è¯­è¨€ä»£ç  ('zh', 'en', 'ja', 'ko')
        """
        if not text:
            return 'unknown'
        
        lang_counts = {}
        
        for lang, pattern in self.lang_patterns.items():
            matches = len(pattern.findall(text))
            if matches > 0:
                lang_counts[lang] = matches
        
        if not lang_counts:
            return 'unknown'
        
        # è¿”å›æœ€å¸¸è§çš„è¯­è¨€
        return max(lang_counts.items(), key=lambda x: x[1])[0]
    
    def calculate_readability(self, text: str) -> float:
        """
        è®¡ç®—å¯è¯»æ€§åˆ†æ•°ï¼ˆåŸºäºFlesch-Kincaidç®€åŒ–ç‰ˆï¼‰
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            å¯è¯»æ€§åˆ†æ•°ï¼ˆ0-100ï¼Œ100æœ€æ˜“è¯»ï¼‰
        """
        # æ¸…ç†æ–‡æœ¬
        text = self.clean_text(text, remove_special=False)
        
        if not text:
            return 0.0
        
        # è®¡ç®—å¥å­æ•°
        sentences = len(re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text))
        
        # è®¡ç®—è¯æ•°
        words = len(re.findall(r'\b[a-zA-Z]+\b', text.lower()))
        
        # è®¡ç®—å­—ç¬¦æ•°ï¼ˆè‹±æ–‡ï¼‰
        chars = len(re.findall(r'[a-zA-Z]', text))
        
        if sentences == 0 or words == 0:
            return 50.0  # é»˜è®¤ä¸­ç­‰å¯è¯»æ€§
        
        # ç®€åŒ–Flesch Reading Ease
        # å¥å­è¶ŠçŸ­ï¼Œè¯è¶Šç®€å•ï¼Œå¯è¯»æ€§è¶Šé«˜
        avg_words_per_sentence = words / sentences
        avg_chars_per_word = chars / words if words > 0 else 0
        
        # ç®€åŒ–å…¬å¼
        score = 206.835 - (1.015 * avg_words_per_sentence) - (84.6 * avg_chars_per_word)
        
        # é™åˆ¶åœ¨0-100ä¹‹é—´
        score = max(0, min(100, score))
        
        return round(score, 2)
    
    def extract_entities(self, text: str) -> List[str]:
        """
        æå–å®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼šæå–å¤§å†™å¼€å¤´è¯å’Œæ•°å­—ï¼‰
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            å®ä½“åˆ—è¡¨
        """
        # æå–å¤§å†™å¼€å¤´çš„è¯ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯ï¼‰
        entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        
        # æå–æ•°å­—ç›¸å…³çš„è¯
        number_entities = re.findall(r'\b\d+(?:\.\d+)?(?:\s*(?:å¹´|æœˆ|æ—¥|ä¸ª|æ¬¡|å…ƒ|ç¾å…ƒ|äººæ°‘å¸))?\b', text)
        
        # æå–å¼•å·ä¸­çš„è¯
        quoted = re.findall(r'["\']([^"\']+)["\']', text)
        
        all_entities = entities + number_entities + quoted
        
        # å»é‡
        seen = set()
        unique_entities = []
        for entity in all_entities:
            if entity not in seen:
                seen.add(entity)
                unique_entities.append(entity)
        
        return unique_entities[:20]  # é™åˆ¶æ•°é‡
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        
        Args:
            text1: ç¬¬ä¸€æ®µæ–‡æœ¬
            text2: ç¬¬äºŒæ®µæ–‡æœ¬
        
        Returns:
            ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        tokens1 = set(self.tokenize(text1))
        tokens2 = set(self.tokenize(text2))
        
        if not tokens1 or not tokens2:
            return 0.0
        
        intersection = tokens1 & tokens2
        union = tokens1 | tokens2
        
        return len(intersection) / len(union)
    
    def analyze(self, text: str) -> TextAnalysisResult:
        """
        å®Œæ•´æ–‡æœ¬åˆ†æ
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            TextAnalysisResult
        """
        cleaned = self.clean_text(text)
        
        result = TextAnalysisResult(
            original_length=len(text),
            cleaned_length=len(cleaned),
            keywords=self.extract_keywords_tfidf(cleaned) + 
                    self.extract_keywords_textrank(cleaned),
            summary=self.summarize(cleaned),
            sentiment=self.analyze_sentiment(cleaned),
            language=self.detect_language(cleaned),
            entities=self.extract_entities(cleaned),
            readability_score=self.calculate_readability(cleaned)
        )
        
        # åˆå¹¶å»é‡å…³é”®è¯
        keyword_dict = {}
        for word, score in result.keywords:
            if word in keyword_dict:
                keyword_dict[word] = max(keyword_dict[word], score)
            else:
                keyword_dict[word] = score
        
        result.keywords = sorted(keyword_dict.items(), 
                                key=lambda x: x[1], reverse=True)[:10]
        
        return result
    
    def format_result(self, result: TextAnalysisResult) -> str:
        """
        æ ¼å¼åŒ–åˆ†æç»“æœ
        
        Args:
            result: åˆ†æç»“æœ
        
        Returns:
            æ ¼å¼åŒ–å­—ç¬¦ä¸²
        """
        output = []
        output.append("=" * 50)
        output.append("ğŸ“Š æ–‡æœ¬åˆ†ææŠ¥å‘Š")
        output.append("=" * 50)
        output.append(f"\nğŸ“ é•¿åº¦: {result.original_length} â†’ {result.cleaned_length} å­—ç¬¦")
        output.append(f"ğŸŒ è¯­è¨€: {result.language}")
        output.append(f"ğŸ“– å¯è¯»æ€§: {result.readability_score}/100")
        
        output.append("\nğŸ”‘ å…³é”®è¯ (Top 10):")
        for i, (word, score) in enumerate(result.keywords, 1):
            output.append(f"   {i}. {word} ({score:.4f})")
        
        output.append("\nğŸ’­ æƒ…æ„Ÿåˆ†æ:")
        output.append(f"   ç§¯æ: {result.sentiment['positive']*100:.1f}%")
        output.append(f"   æ¶ˆæ: {result.sentiment['negative']*100:.1f}%")
        output.append(f"   ä¸­æ€§: {result.sentiment['neutral']*100:.1f}%")
        
        output.append("\nğŸ·ï¸ è¯†åˆ«çš„å®ä½“:")
        if result.entities:
            for entity in result.entities[:10]:
                output.append(f"   â€¢ {entity}")
        else:
            output.append("   (æœªè¯†åˆ«åˆ°å®ä½“)")
        
        output.append("\nğŸ“ æ‘˜è¦:")
        output.append(f"   {result.summary[:200]}{'...' if len(result.summary) > 200 else ''}")
        
        output.append("\n" + "=" * 50)
        
        return '\n'.join(output)


def demo():
    """æ¼”ç¤º"""
    processor = SmartTextProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
        å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡º
        ååº”çš„æ™ºèƒ½æœºå™¨ã€‚ç ”ç©¶èŒƒå›´åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€
        ä¸“å®¶ç³»ç»Ÿç­‰é¢†åŸŸã€‚äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ï¼Œ
        å›¾çµæå‡ºäº†è‘—åçš„"å›¾çµæµ‹è¯•"ï¼Œæˆä¸ºåˆ¤æ–­æœºå™¨æ˜¯å¦å…·æœ‰æ™ºèƒ½çš„é‡è¦æ ‡å‡†ã€‚
        è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„çªç ´ä½¿å¾—äººå·¥æ™ºèƒ½åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€
        è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚
        """,
        """
        The quick brown fox jumps over the lazy dog. This is a sample text
        for testing the English language processing capabilities of our
        smart text processor. Natural Language Processing (NLP) is a fascinating
        field of artificial intelligence that focuses on the interaction
        between computers and human language.
        """
    ]
    
    print("ğŸ§ª Smart Text Processor Demo")
    print("=" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“„ æµ‹è¯•æ–‡æœ¬ {i}:")
        print("-" * 30)
        print(text[:100] + "..." if len(text) > 100 else text)
        
        result = processor.analyze(text)
        print(processor.format_result(result))
        print()
    
    # æµ‹è¯•ç›¸ä¼¼åº¦
    text1 = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
    text2 = "AIæŠ€æœ¯æ­£åœ¨é©æ–°å…¨çƒ"
    similarity = processor.calculate_similarity(text1, text2)
    print(f"\nğŸ”— ç›¸ä¼¼åº¦æµ‹è¯•: '{text1}' vs '{text2}'")
    print(f"   ç›¸ä¼¼åº¦: {similarity:.4f}")


def batch_process(texts: List[str]) -> List[TextAnalysisResult]:
    """
    æ‰¹é‡å¤„ç†æ–‡æœ¬
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
    
    Returns:
        åˆ†æç»“æœåˆ—è¡¨
    """
    processor = SmartTextProcessor()
    return [processor.analyze(text) for text in texts]


if __name__ == "__main__":
    demo()

#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨
Smart Text Summarizer

æ”¯æŒå¤šç§æ‘˜è¦ç®—æ³•ï¼šExtractive/Abstractive/å…³é”®è¯æå–
åŸºäº TextRankã€TF-IDF å’Œ LLM çš„æ™ºèƒ½æ‘˜è¦
"""

import re
import json
import hashlib
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from enum import Enum
import math


class SummarizationMethod(Enum):
    TEXTRANK = "textrank"
    TF_IDF = "tfidf"
    EXTRACTIVE_LLM = "extract_llm"
    ABSTRACTIVE_LLM = "abstract_llm"
    HYBRID = "hybrid"


class TextType(Enum):
    NEWS = "news"
    ACADEMIC = "academic"
    TECHNICAL = "technical"
    CONVERSATION = "conversation"
    GENERAL = "general"


@dataclass
class Sentence:
    """å¥å­ç»“æ„"""
    text: str
    index: int
    score: float = 0.0
    words: List[str] = None
    
    def __post_init__(self):
        if self.words is None:
            self.words = self.text.lower().split()


@dataclass
class SummaryResult:
    """æ‘˜è¦ç»“æœ"""
    summary: str
    method: SummarizationMethod
    compression_ratio: float
    key_points: List[str]
    confidence: float
    processing_time_ms: float


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†"""
    
    # åœç”¨è¯åˆ—è¡¨
    STOPWORDS = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
        'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
        'è‡ªå·±', 'è¿™', 'é‚£', 'ä¹ˆ', 'å¥¹', 'ä»–', 'å®ƒ', 'ä»¬', 'ä¸º', 'ä»€ä¹ˆ', 'å¯ä»¥', 'è¿˜',
        'from', 'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'in', 'to', 'of',
        'it', 'for', 'with', 'as', 'be', 'are', 'was', 'were', 'this', 'that', 'by'
    }
    
    # URLæ­£åˆ™
    URL_PATTERN = re.compile(r'https?://\S+|www\.\S+')
    
    # é‚®ç®±æ­£åˆ™
    EMAIL_PATTERN = re.compile(r'\S+@\S+\.\S+')
    
    @classmethod
    def preprocess(cls, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # ç§»é™¤URL
        text = cls.URL_PATTERN.sub('', text)
        # ç§»é™¤é‚®ç®±
        text = cls.EMAIL_PATTERN.sub('', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™æ ‡ç‚¹
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\'\"]', '', text)
        return text.strip()
    
    @classmethod
    def split_sentences(cls, text: str) -> List[Sentence]:
        """åˆ†å‰²å¥å­"""
        # ä¸­æ–‡å¥å­åˆ†å‰²
        chinese_pattern = r'[ã€‚ï¼ï¼Ÿï¼›]'
        # è‹±æ–‡å¥å­åˆ†å‰²
        english_pattern = r'[.!?;]'
        
        # åˆå¹¶åˆ†å‰²
        sentences = []
        sentence_list = re.split(chinese_pattern + '|' + english_pattern, text)
        
        for idx, sent in enumerate(sentence_list):
            sent = sent.strip()
            if len(sent) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                sentences.append(Sentence(
                    text=sent,
                    index=idx
                ))
        
        return sentences
    
    @classmethod
    def extract_words(cls, text: str) -> List[str]:
        """æå–è¯æ±‡"""
        words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]+\b', text.lower())
        return [w for w in words if w not in cls.STOPWORDS and len(w) > 1]


class TFIDFScorer:
    """TF-IDFè¯„åˆ†å™¨"""
    
    def __init__(self):
        self.doc_freq: Dict[str, int] = {}
        self.total_docs = 0
    
    def fit(self, sentences: List[Sentence]):
        """æ‹Ÿåˆè¯­æ–™åº“"""
        self.total_docs = len(sentences)
        
        for sent in sentences:
            words = set(sent.words)
            for word in words:
                self.doc_freq[word] = self.doc_freq.get(word, 0) + 1
    
    def score(self, sentence: Sentence) -> float:
        """è®¡ç®—TF-IDFåˆ†æ•°"""
        if self.total_docs == 0:
            return 0.0
        
        words = sentence.words
        if not words:
            return 0.0
        
        # TFè®¡ç®—
        word_count = {}
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
        
        tf_score = sum(1 + math.log(count) for count in word_count.values()) / len(words)
        
        # IDFè®¡ç®—
        idf_score = 0
        for word in set(words):
            df = self.doc_freq.get(word, 0)
            if df > 0:
                idf_score += math.log(self.total_docs / df)
        
        return tf_score * idf_score / max(len(set(words)), 1)


class TextRankScorer:
    """TextRankè¯„åˆ†å™¨"""
    
    def __init__(self, damping: float = 0.85, max_iter: int = 100, tolerance: float = 1e-4):
        self.damping = damping
        self.max_iter = max_iter
        self.tolerance = tolerance
    
    def _similarity(self, sent1: Sentence, sent2: Sentence) -> float:
        """è®¡ç®—å¥å­ç›¸ä¼¼åº¦ (Jaccard)"""
        set1 = set(sent1.words)
        set2 = set(sent2.words)
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def _build_graph(self, sentences: List[Sentence]) -> List[List[float]]:
        """æ„å»ºç›¸ä¼¼åº¦å›¾"""
        n = len(sentences)
        graph = [[0.0] * n for _ in range(n)]
        
        for i in range(n):
            for j in range(i + 1, n):
                sim = self._similarity(sentences[i], sentences[j])
                graph[i][j] = sim
                graph[j][i] = sim
        
        return graph
    
    def score(self, sentences: List[Sentence]) -> List[float]:
        """TextRankè¿­ä»£è®¡ç®—"""
        if not sentences:
            return []
        
        n = len(sentences)
        graph = self._build_graph(sentences)
        
        # å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
        out_weights = [sum(row) for row in graph]
        norm_graph = [[w / out_weights[i] if out_weights[i] > 0 else 0 
                      for w in row] for i, row in enumerate(graph)]
        
        # åˆå§‹åŒ–åˆ†æ•°
        scores = [1.0 / n] * n
        
        # è¿­ä»£è®¡ç®—
        for _ in range(self.max_iter):
            new_scores = []
            for i in range(n):
                score = (1 - self.damping) / n
                for j in range(n):
                    if i != j:
                        score += self.damping * norm_graph[j][i] * scores[j]
                new_scores.append(score)
            
            # æ£€æŸ¥æ”¶æ•›
            diff = sum(abs(s - ns) for s, ns in zip(scores, new_scores))
            scores = new_scores
            
            if diff < self.tolerance:
                break
        
        return scores


class KeywordExtractor:
    """å…³é”®è¯æå–å™¨"""
    
    def __init__(self, top_k: int = 10):
        self.top_k = top_k
    
    def extract(self, text: str) -> List[Tuple[str, float]]:
        """æå–å…³é”®è¯"""
        words = TextPreprocessor.extract_words(text)
        word_freq = {}
        
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # è®¡ç®—é¢‘ç‡åˆ†æ•°
        max_freq = max(word_freq.values()) if word_freq else 1
        
        keywords = []
        for word, freq in word_freq.items():
            score = freq / max_freq
            keywords.append((word, score))
        
        # æ’åºè¿”å›
        keywords.sort(key=lambda x: x[1], reverse=True)
        return keywords[:self.top_k]


class SmartTextSummarizer:
    """æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨"""
    
    def __init__(self, method: SummarizationMethod = SummarizationMethod.HYBRID):
        self.method = method
        self.textrank_scorer = TextRankScorer()
        self.tfidf_scorer = TFIDFScorer()
        self.keyword_extractor = KeywordExtractor()
    
    def detect_text_type(self, text: str) -> TextType:
        """æ£€æµ‹æ–‡æœ¬ç±»å‹"""
        text_lower = text.lower()
        
        # å­¦æœ¯è®ºæ–‡ç‰¹å¾
        academic_markers = ['abstract', 'introduction', 'methodology', 'conclusion', 
                          'å‚è€ƒæ–‡çŒ®', 'è®ºæ–‡', 'ç ”ç©¶', 'å®éªŒ']
        if any(marker in text_lower for marker in academic_markers):
            return TextType.ACADEMIC
        
        # æŠ€æœ¯æ–‡æ¡£ç‰¹å¾
        tech_markers = ['function', 'class', 'api', 'method', 'parameter', 
                       'def ', 'import ', 'class ', '//', '```']
        if any(marker in text_lower for marker in tech_markers):
            return TextType.TECHNICAL
        
        # æ–°é—»ç‰¹å¾
        news_markers = ['æŠ¥é“', 'è®°è€…', 'æ–°åç¤¾', 'åŒ—äº¬', 'åç››é¡¿', 'æ®æ–°åç¤¾']
        if any(marker in text_lower for marker in news_markers):
            return TextType.NEWS
        
        return TextType.GENERAL
    
    def _extractive_summary(self, sentences: List[Sentence], 
                           scores: List[float],
                           max_length: int = 500,
                           min_sentences: int = 2) -> str:
        """æŠ½å–å¼æ‘˜è¦"""
        # æŒ‰åˆ†æ•°æ’åº
        scored = [(s, sc) for s, sc in zip(sentences, scores) if sc > 0]
        scored.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©é«˜åˆ†å¥å­ï¼Œä¿æŒåŸå§‹é¡ºåº
        selected = []
        current_length = 0
        
        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åº
        for sent, score in scored:
            if len(selected) >= min_sentences:
                if current_length >= max_length * 0.8:
                    break
            
            if sent not in selected:
                selected.append(sent)
                current_length += len(sent.text)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åˆ—
        selected.sort(key=lambda x: x.index)
        
        return 'ã€‚'.join(s.text for s in selected) + 'ã€‚' if selected else ''
    
    def _generate_key_points(self, sentences: List[Sentence], 
                            scores: List[float],
                            top_n: int = 5) -> List[str]:
        """ç”Ÿæˆå…³é”®ç‚¹"""
        scored = list(zip(sentences, scores))
        scored.sort(key=lambda x: x[1], reverse=True)
        
        key_points = []
        for sent, score in scored[:top_n]:
            if score > 0:
                # ç®€åŒ–å¥å­
                point = sent.text[:100] + '...' if len(sent.text) > 100 else sent.text
                key_points.append(point)
        
        return key_points
    
    def summarize(self, text: str, 
                 max_length: int = 500,
                 min_sentences: int = 2,
                 method: Optional[SummarizationMethod] = None) -> SummaryResult:
        """ç”Ÿæˆæ‘˜è¦"""
        import time
        start_time = time.time()
        
        use_method = method or self.method
        
        # é¢„å¤„ç†
        clean_text = TextPreprocessor.preprocess(text)
        sentences = TextPreprocessor.split_sentences(clean_text)
        
        if not sentences:
            return SummaryResult(
                summary="æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼šæ–‡æœ¬å¤ªçŸ­æˆ–æ— æ•ˆ",
                method=use_method,
                compression_ratio=0.0,
                key_points=[],
                confidence=0.0,
                processing_time_ms=0.0
            )
        
        # TF-IDFè¯„åˆ†
        self.tfidf_scorer.fit(sentences)
        tfidf_scores = [self.tfidf_scorer.score(s) for s in sentences]
        
        # TextRankè¯„åˆ†
        textrank_scores = self.textrank_scorer.score(sentences)
        
        # æ ¹æ®æ–¹æ³•ç»„åˆåˆ†æ•°
        if use_method == SummarizationMethod.TEXTRANK:
            final_scores = textrank_scores
        elif use_method == SummarizationMethod.TF_IDF:
            final_scores = tfidf_scores
        else:  # HYBRID
            final_scores = [
                0.5 * t + 0.5 * f 
                for t, f in zip(textrank_scores, tfidf_scores)
            ]
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._extractive_summary(sentences, final_scores, max_length, min_sentences)
        
        # ç”Ÿæˆå…³é”®ç‚¹
        key_points = self._generate_key_points(sentences, final_scores)
        
        # è®¡ç®—å‹ç¼©æ¯”
        original_length = len(clean_text)
        summary_length = len(summary)
        compression_ratio = summary_length / original_length if original_length > 0 else 0
        
        # è®¡ç®—ç½®ä¿¡åº¦
        top_score = max(final_scores) if final_scores else 0
        confidence = min(0.9, top_score / max(sum(textrank_scores), sum(tfidf_scores)) * 10) if (sum(textrank_scores) + sum(tfidf_scores)) > 0 else 0.5
        
        processing_time = (time.time() - start_time) * 1000
        
        return SummaryResult(
            summary=summary if summary else "æ–‡æœ¬å¤ªçŸ­ï¼Œæ— éœ€æ‘˜è¦",
            method=use_method,
            compression_ratio=compression_ratio,
            key_points=key_points,
            confidence=confidence,
            processing_time_ms=processing_time
        )
    
    def analyze(self, text: str) -> Dict:
        """å…¨é¢åˆ†ææ–‡æœ¬"""
        text_type = self.detect_text_type(text)
        keywords = self.keyword_extractor.extract(text)
        
        summary = self.summarize(text)
        
        return {
            "text_type": text_type.value,
            "keywords": [{"word": k, "score": v} for k, v in keywords],
            "summary": summary.summary,
            "compression_ratio": f"{summary.compression_ratio:.1%}",
            "confidence": f"{summary.confidence:.2f}",
            "key_points": summary.key_points
        }


def demo():
    """æ¼”ç¤º"""
    print("=" * 60)
    print("ğŸ§  æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ - æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = {
        "news": """æ–°åç¤¾åŒ—äº¬2æœˆ3æ—¥ç”µ è¿‘æ—¥ï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸä¼ æ¥é‡ç£…æ¶ˆæ¯ã€‚
å¤šä¸ªç§‘æŠ€å·¨å¤´å®£å¸ƒåŠ å¤§åœ¨ç”Ÿæˆå¼AIé¢†åŸŸçš„æŠ•å…¥ã€‚
ä¸“å®¶è¡¨ç¤ºï¼Œè¿™å°†æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ã€‚
é¢„è®¡æœªæ¥å‡ å¹´ï¼ŒAIå°†åœ¨åŒ»ç–—ã€æ•™è‚²ã€é‡‘èç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚
ä¸æ­¤åŒæ—¶ï¼Œå„å›½æ”¿åºœä¹Ÿåœ¨ç§¯æåˆ¶å®šAIç›‘ç®¡æ”¿ç­–ã€‚
åˆ†æè®¤ä¸ºï¼Œåœ¨åˆ›æ–°ä¸ç›‘ç®¡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡è‡³å…³é‡è¦ã€‚""",
        
        "tech": """æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ã€‚
å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
ç›‘ç£å­¦ä¹ éœ€è¦æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚
æ— ç›‘ç£å­¦ä¹ åˆ™ç”¨äºå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚
è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—çªç ´ã€‚""",
        
        "academic": """æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ã€‚
é¦–å…ˆï¼Œæˆ‘ä»¬å›é¡¾äº†ç›¸å…³å·¥ä½œçš„ç†è®ºåŸºç¡€ã€‚
ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚
å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚
æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ‰©å±•æ¨¡å‹è§„æ¨¡å’Œæ¢ç´¢æ–°çš„è®­ç»ƒç­–ç•¥ã€‚"""
    }
    
    summarizer = SmartTextSummarizer()
    
    for name, text in test_texts.items():
        print(f"\nğŸ“ {name.upper()} æ–‡æœ¬æ‘˜è¦:")
        print("-" * 40)
        
        result = summarizer.summarize(text)
        
        print(f"æ‘˜è¦: {result.summary[:150]}...")
        print(f"å‹ç¼©æ¯”: {result.compression_ratio:.1%}")
        print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
        print(f"å¤„ç†æ—¶é—´: {result.processing_time_ms:.1f}ms")
    
    # å®Œæ•´åˆ†ææ¼”ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š æ–‡æœ¬åˆ†ææ¼”ç¤º")
    print("=" * 60)
    
    analysis = summarizer.analyze(test_texts["tech"])
    print(f"\næ–‡æœ¬ç±»å‹: {analysis['text_type']}")
    print(f"\nå…³é”®è¯:")
    for kw in analysis["keywords"][:5]:
        print(f"  â€¢ {kw['word']}: {kw['score']:.3f}")
    print(f"\nå…³é”®è¦ç‚¹:")
    for i, point in enumerate(analysis['key_points'], 1):
        print(f"  {i}. {point[:60]}...")
    
    print("\n" + "=" * 60)
    print("âœ¨ æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    demo()

#!/usr/bin/env python3
"""
GitHub Trending Repository Crawler
GitHubè¶‹åŠ¿ä»“åº“çˆ¬è™«å·¥å…· - è‡ªåŠ¨æŠ“å–GitHubçƒ­é—¨ä»“åº“

Author: MarsAssistant
Day: 80
"""

import requests
import json
import csv
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time


class GitHubTrending:
    """GitHub Trending çˆ¬è™«ç±»"""
    
    BASE_URL = "https://api.github.com"
    TRENDING_URL = "https://github.com/trending"
    
    # æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    SUPPORTED_LANGUAGES = {
        'python': 'Python',
        'javascript': 'JavaScript', 
        'typescript': 'TypeScript',
        'java': 'Java',
        'go': 'Go',
        'rust': 'Rust',
        'cpp': 'C++',
        'c': 'C',
        'ruby': 'Ruby',
        'php': 'PHP',
        'swift': 'Swift',
        'kotlin': 'Kotlin',
        'scala': 'Scala',
        'shell': 'Shell',
        'vue': 'Vue',
        'angular': 'Angular',
        'react': 'React',
        'jupyter-notebook': 'Jupyter Notebook'
    }
    
    def __init__(self, token: Optional[str] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            token: GitHub Personal Access Token (å¯é€‰ï¼Œç”¨äºæé«˜APIé™åˆ¶)
        """
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'GitHub-Trending-Crawler/1.0'
        })
        
        if token:
            self.session.headers['Authorization'] = f'token {token}'
        
        # APIé™åˆ¶è¿½è¸ª
        self.rate_limit_remaining = float('inf')
        self.rate_limit_reset = None
    
    def _check_rate_limit(self):
        """æ£€æŸ¥å¹¶æ›´æ–°APIé™åˆ¶"""
        try:
            response = self.session.get(f"{self.BASE_URL}/rate_limit")
            if response.status_code == 200:
                data = response.json()
                # æœç´¢APIçš„é™åˆ¶
                search_limit = data.get('resources', {}).get('search', {})
                self.rate_limit_remaining = search_limit.get('remaining', float('inf'))
                self.rate_limit_reset = search_limit.get('reset')
        except:
            pass
    
    def search_trending_repos(
        self,
        language: str = None,
        created_since: str = None,
        stars: str = None,
        per_page: int = 30,
        page: int = 1
    ) -> List[Dict]:
        """
        æœç´¢è¶‹åŠ¿ä»“åº“
        
        Args:
            language: ç¼–ç¨‹è¯­è¨€
            created_since: åˆ›å»ºæ—¶é—´ (daily/weekly/monthly)
            stars: æœ€å°‘starsæ•°é‡
            per_page: æ¯é¡µæ•°é‡
            page: é¡µç 
            
        Returns:
            ä»“åº“åˆ—è¡¨
        """
        # æ„å»ºæŸ¥è¯¢
        query_parts = []
        
        if language and language.lower() in self.SUPPORTED_LANGUAGES:
            query_parts.append(f"language:{language}")
        
        if created_since:
            # å°† daily/weekly/monthly è½¬æ¢ä¸ºæ—¥æœŸ
            days_map = {
                'daily': 1,
                'weekly': 7,
                'monthly': 30
            }
            days = days_map.get(created_since.lower(), 1)
            since_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            query_parts.append(f"created:>={since_date}")
        
        if stars:
            query_parts.append(f"stars:>={stars}")
        
        # æ’åº
        query_parts.append("sort:stars")
        
        query = " ".join(query_parts)
        
        # APIè°ƒç”¨
        url = f"{self.BASE_URL}/search/repositories"
        params = {
            'q': query,
            'sort': 'stars',
            'order': 'desc',
            'per_page': min(per_page, 100),
            'page': page
        }
        
        try:
            response = self.session.get(url, params=params)
            
            # æ›´æ–°é€Ÿç‡é™åˆ¶
            self.rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
            reset_time = response.headers.get('X-RateLimit-Reset')
            if reset_time:
                self.rate_limit_reset = int(reset_time)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('items', [])
            elif response.status_code == 403:
                print("âš ï¸ APIé€Ÿç‡é™åˆ¶å·²è¾¾åˆ°ï¼Œè¯·ç¨åå†è¯•æˆ–æä¾›token")
                return []
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return []
    
    def parse_trending_page(self, language: str = None, timeframe: str = 'daily') -> List[Dict]:
        """
        è§£æGitHub Trendingé¡µé¢ (å¤‡ç”¨æ–¹æ¡ˆ)
        
        Args:
            language: ç¼–ç¨‹è¯­è¨€
            timeframe: æ—¶é—´èŒƒå›´ (daily/weekly/monthly)
            
        Returns:
            ä»“åº“åˆ—è¡¨
        """
        url = self.TRENDING_URL
        if language:
            url += f"/{language}"
        url += f"?since={timeframe}"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                # ç®€åŒ–è§£æ - è¿”å›URLåˆ—è¡¨
                import re
                repo_pattern = r'/[a-zA-Z0-9-]+/[a-zA-Z0-9-]+'
                repos = re.findall(repo_pattern, response.text)
                
                # å»é‡
                unique_repos = list(set(repos))[:25]
                
                return [{
                    'full_name': repo.strip('/'),
                    'html_url': f"https://github.com{repo}"
                } for repo in unique_repos]
            
            return []
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            return []
    
    def get_repo_details(self, owner: str, repo: str) -> Optional[Dict]:
        """
        è·å–ä»“åº“è¯¦ç»†ä¿¡æ¯
        
        Args:
            owner: ä»“åº“æ‰€æœ‰è€…
            repo: ä»“åº“å
            
        Returns:
            ä»“åº“è¯¦æƒ…å­—å…¸
        """
        url = f"{self.BASE_URL}/repos/{owner}/{repo}"
        
        try:
            response = self.session.get(url)
            
            if response.status_code == 200:
                return response.json()
            return None
            
        except Exception as e:
            print(f"âŒ è·å–ä»“åº“è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def analyze_trending(self, repos: List[Dict]) -> Dict:
        """
        åˆ†æè¶‹åŠ¿æ•°æ®
        
        Args:
            repos: ä»“åº“åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœ
        """
        if not repos:
            return {}
        
        languages = {}
        total_stars = 0
        total_forks = 0
        has_description = 0
        
        for repo in repos:
            total_stars += repo.get('stargazers_count', 0)
            total_forks += repo.get('forks_count', 0)
            
            lang = repo.get('language', 'Unknown')
            languages[lang] = languages.get(lang, 0) + 1
            
            if repo.get('description'):
                has_description += 1
        
        return {
            'total_repos': len(repos),
            'total_stars': total_stars,
            'avg_stars': round(total_stars / len(repos), 1),
            'total_forks': total_forks,
            'languages': dict(sorted(languages.items(), key=lambda x: x[1], reverse=True)),
            'description_coverage': round(has_description / len(repos) * 100, 1)
        }
    
    def export_json(self, repos: List[Dict], filename: str):
        """
        å¯¼å‡ºä¸ºJSONæ ¼å¼
        
        Args:
            repos: ä»“åº“åˆ—è¡¨
            filename: æ–‡ä»¶å
        """
        output = {
            'generated_at': datetime.now().isoformat(),
            'total_count': len(repos),
            'repos': repos
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²å¯¼å‡ºJSON: {filename}")
    
    def export_csv(self, repos: List[Dict], filename: str):
        """
        å¯¼å‡ºä¸ºCSVæ ¼å¼
        
        Args:
            repos: ä»“åº“åˆ—è¡¨
            filename: æ–‡ä»¶å
        """
        if not repos:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        fieldnames = ['full_name', 'html_url', 'description', 'language', 
                      'stargazers_count', 'forks_count', 'open_issues_count',
                      'created_at', 'updated_at', 'owner.login']
        
        with open(filename, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for repo in repos:
                row = {field: repo.get(field, '') for field in fieldnames}
                writer.writerow(row)
        
        print(f"âœ… å·²å¯¼å‡ºCSV: {filename}")
    
    def export_markdown(self, repos: List[Dict], filename: str, title: str = None):
        """
        å¯¼å‡ºä¸ºMarkdownæ ¼å¼
        
        Args:
            repos: ä»“åº“åˆ—è¡¨
            filename: æ–‡ä»¶å
            title: æ ‡é¢˜
        """
        if title is None:
            title = f"GitHub Trending Repositories - {datetime.now().strftime('%Y-%m-%d')}"
        
        md_lines = [
            f"# {title}\n",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            f"æ€»æ•°é‡: {len(repos)}\n",
            "---\n\n"
        ]
        
        for i, repo in enumerate(repos, 1):
            md_lines.append(f"## {i}. {repo.get('full_name', 'Unknown')}\n")
            md_lines.append(f"- â­ Stars: {repo.get('stargazers_count', 0)}")
            md_lines.append(f" | ğŸ´ Forks: {repo.get('forks_count', 0)}")
            md_lines.append(f" | ğŸ› Issues: {repo.get('open_issues_count', 0)}\n")
            md_lines.append(f"- ğŸ·ï¸ Language: {repo.get('language', 'N/A')}\n")
            
            if repo.get('description'):
                md_lines.append(f"\nğŸ“ {repo.get('description')}\n")
            
            md_lines.append(f"\nğŸ”— [View on GitHub]({repo.get('html_url', '')})\n")
            md_lines.append("\n---\n\n")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(''.join(md_lines))
        
        print(f"âœ… å·²å¯¼å‡ºMarkdown: {filename}")


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ GitHub Trending Crawler Demo")
    print("=" * 50)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = GitHubTrending()
    
    # æ£€æŸ¥é€Ÿç‡é™åˆ¶
    print("\nğŸ“Š æ£€æŸ¥APIé€Ÿç‡é™åˆ¶...")
    crawler._check_rate_limit()
    print(f"å‰©ä½™è¯·æ±‚: {crawler.rate_limit_remaining}")
    
    # æœç´¢è¶‹åŠ¿ä»“åº“ (Pythonè¯­è¨€)
    print("\nğŸ” æœç´¢Pythonè¶‹åŠ¿ä»“åº“...")
    repos = crawler.search_trending_repos(
        language='python',
        stars='100',
        per_page=10
    )
    
    if repos:
        print(f"\nâœ… æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“\n")
        
        # æ˜¾ç¤ºå‰5ä¸ª
        for i, repo in enumerate(repos[:5], 1):
            print(f"{i}. {repo.get('full_name')}")
            print(f"   â­ {repo.get('stargazers_count')} | ğŸ´ {repo.get('forks_count')}")
            print(f"   ğŸ·ï¸ {repo.get('language')}")
            print()
        
        # åˆ†æç»“æœ
        analysis = crawler.analyze_trending(repos)
        print("ğŸ“ˆ æ•°æ®åˆ†æ:")
        print(f"   - æ€»Stars: {analysis.get('total_stars')}")
        print(f"   - å¹³å‡Stars: {analysis.get('avg_stars')}")
        print(f"   - è¯­è¨€åˆ†å¸ƒ: {analysis.get('languages')}")
        
        # å¯¼å‡ºæ–‡ä»¶
        print("\nğŸ’¾ å¯¼å‡ºæ•°æ®...")
        crawler.export_json(repos, 'trending_repos.json')
        crawler.export_csv(repos, 'trending_repos.csv')
        crawler.export_markdown(repos, 'trending_repos.md')
    else:
        print("âŒ æœªæ‰¾åˆ°ä»“åº“æˆ–APIé™åˆ¶")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='GitHub Trending Repository Crawler',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python github_trending.py --language python --stars 100
  python github_trending.py --language javascript --output json
  python github_trending.py --language go --timeframe weekly
        """
    )
    
    parser.add_argument('-l', '--language', 
                        help='ç¼–ç¨‹è¯­è¨€ (python/javascript/go/rustç­‰)')
    parser.add_argument('-s', '--stars', type=int, default=100,
                        help='æœ€å°‘starsæ•°é‡ (é»˜è®¤: 100)')
    parser.add_argument('-p', '--per-page', type=int, default=30,
                        help='æ¯é¡µæ•°é‡ (é»˜è®¤: 30)')
    parser.add_argument('-o', '--output', 
                        choices=['json', 'csv', 'markdown', 'all'],
                        default='all',
                        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: all)')
    parser.add_argument('-t', '--timeframe',
                        choices=['daily', 'weekly', 'monthly'],
                        default='daily',
                        help='æ—¶é—´èŒƒå›´ (é»˜è®¤: daily)')
    parser.add_argument('--token',
                        help='GitHub Personal Access Token (å¯é€‰)')
    parser.add_argument('--demo', action='store_true',
                        help='è¿è¡Œæ¼”ç¤º')
    
    args = parser.parse_args()
    
    if args.demo:
        demo()
        return
    
    # åˆ›å»ºçˆ¬è™«
    crawler = GitHubTrending(token=args.token)
    
    print(f"ğŸ” æœç´¢è¶‹åŠ¿ä»“åº“: {args.language or 'å…¨éƒ¨'} | Stars â‰¥ {args.stars}")
    
    # æœç´¢
    repos = crawler.search_trending_repos(
        language=args.language,
        stars=str(args.stars),
        per_page=args.per_page
    )
    
    if repos:
        print(f"\nâœ… æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“\n")
        
        # åˆ†æ
        analysis = crawler.analyze_trending(repos)
        print("ğŸ“ˆ ç»Ÿè®¡:")
        print(f"   æ€»Stars: {analysis.get('total_stars')}")
        print(f"   å¹³å‡Stars: {analysis.get('avg_stars')}")
        print(f"   è¯­è¨€åˆ†å¸ƒ: {list(analysis.get('languages', {}).items())[:5]}")
        
        # å¯¼å‡º
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        lang_prefix = args.language or 'all'
        
        if args.output in ['json', 'all']:
            crawler.export_json(repos, f'trending_{lang_prefix}_{timestamp}.json')
        if args.output in ['csv', 'all']:
            crawler.export_csv(repos, f'trending_{lang_prefix}_{timestamp}.csv')
        if args.output in ['markdown', 'all']:
            crawler.export_markdown(repos, f'trending_{lang_prefix}_{timestamp}.md')
    else:
        print("âŒ æœªæ‰¾åˆ°ä»“åº“")


if __name__ == '__main__':
    main()
